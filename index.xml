<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Red Hat | Public Sector</title>
    <link>/</link>
    <description>Recent content on Red Hat | Public Sector</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/jr-sa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/jr-sa/</guid>
      <description>.display-1 { font-size: 32px; font-family: var(--pfe-theme--font-family--heading,RedHatDisplay,Overpass,Arial,sans-serif); } p { font-size: 14px; } li { font-size: 14px; }  Graduating in 2021? Red Hat’s North American Public Sector Team is hiring! JUNIOR SOLUTIONS ARCHITECTS  Do rewarding work in an award-winning company                        What we are looking for:  Students graduating in May 2021 Strong desire to learn new skills and general curiosity for new technologies Excellent communication skills including written, verbal, and presentation skills Must be comfortable presenting to small and large groups of mixed audiences Experience with open source projects   The following are a plus:  US citizenship preferred, as the position may require working with US government customers Red Hat Certified System Administrator (RHCSA), VMware Certified Professional, or ITIL certifications Experience working Red Hat products or its upstream open source projects such as Linux, Ansible, and Kubernetes     Available work locations Raleigh, NC Washington, DC Relocation may be required.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/readme/</guid>
      <description>Introduction Red Hat Certificate System is an enterprise software system that gives you a scalable, secure framework to establish and maintain trusted identities and keep communications private.
Red Hat Certificate System provides certificate life-cycle management-issue, renew, suspend, revoke, archive and recover, and manage single and dual-key X.509v3 certificates needed to handle strong authentication, single sign-on, and secure communications.
The upstream release is called Dogtag.
Curriculum overview  Unit 1: Installing Directory Server Unit 2: Installing a Certificate Authority Unit 3: Installing other subsytems Unit 4: Accessing the Web Interface Unit 5: CLI tools Unit 6: Issuing a certificate Unit 7: Recovering a certificate Unit 8: Revoking a certificate Unit 9: OCSP Responder Unit 10: Token Management System Unit 11: Certificate profiles Unit 12: Troubleshooting  Preparation Everything is installed on one (1) VM or physical machine but some preparation is needed prior to starting this workshop.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_1/</guid>
      <description>Unit 1: Installing Directory Server In this unit you will install Red Hat Directory Server. Red Hat Directory Server provides a centralized directory service for an intranet, network, and extranet information. Directory Server integrates with existing systems and acts as a centralized repository for the consolidation of employee, customer, supplier, and partner information. Directory Server can even be extended to manage user profiles, preferences, and authentication.
Red Hat Directory Server is used as the backend for Red Hat Certificate System.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_10/</guid>
      <description>Unit 10: Token Management System In this unit you will login to the TPS web interface, add a new user, and generate a virtual token.
TPS Web Interface The admin certificate needs to be in your browser. This was completed during Unit 4.
 Open the TPS webpage.
https://ca1.redhat.example.com:8443/tps/
 Click the Log In button.
 Go through the different tabs to become familiar with the interface. It will be mostly blank until we create users and tokens.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_11/</guid>
      <description>Unit 11: Certificate profiles In this unit you will go over the caUserCert profile. This certificate profile is for enrolling user certificates.
The CA (certificate authority) profiles are located here:
/var/lib/pki/ca1/ca/profiles/ca The caUserCert.cfg profile is located in the rhcs labs folder.
Example caUserCert Profile The fields you will see in a CA User Profile are as follows:
desc is a long description of the user certificate profile visible (true/false) controls whether the certificate will be visible to users enable, when selected (true), will allow the certificate to be used enableBy identifies the user who enabled the certificate name is the short name used for the certificate that will be viewable by users</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_12/</guid>
      <description>Unit 12: Troubleshooting In this unit you will troubleshoot common issues with maintaining a Certificate System. Each section has a shell script to run that will &amp;quot;break&amp;quot; the system. A hint will be offered to help you hone your troubleshooting skills.
Instructions To solve, the CA should be accessible in a browser without errors in status output or logs.
Run script to break CA (1st lab)  Change to labs troubleshooting directory.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_2/</guid>
      <description>Unit 2: Installing a Certificate Authority In this unit you will install a Certificate Authority (CA), or Certificate Manager. The CA is the core of the PKI; it issues and revokes all certificates. The Certificate Manager is also the core of the Certificate System. By establishing a security domain of trusted subsystems, it establishes and manages relationships between the other subsystems.
The pkispawn setup script can be used interactively but we are going to use an automated install with pre-configured settings.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_3/</guid>
      <description>Unit 3: Installing other subsytems In this unit you will install the other subsystems associated with Certificate System. Key Recovery Authority (KRA), Online Certificate Status Protocol (OCSP), Token Key Service (TKS), and Token Processing System (TPS).
KRA A Key Recovery Authority (KRA). Certificates are created based on a specific and unique key pair. If a private key is ever lost, then the data which that key was used to access (such as encrypted emails) is also lost because it is inaccessible.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_4/</guid>
      <description>Unit 4: Accessing the Web Interface In this unit you will access the web interface of each subsystem. Each Certificate System instance is a Tomcat server instance and each Certificate System subsystem (such as CA or KRA) is deployed as a web application in Tomcat.
All subsystems share a common administrative interface and have an agent interface that allows for agents to perform the tasks assigned to them. A CA Subsystem has an end-entity interface that allows end-entities to enroll in the PKI.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_5/</guid>
      <description>Unit 5: CLI tools In this unit you will access the Certificate System using various command-line interface (CLI) tools. Red Hat Certificate System provides a client command-line interface to access various services on the server.
Prerequisites Your environment needs to be configured to use the ca_admin certificate to interact with the subsystems via the CLI. The root user does not have be to used but access to /root/.dogtag is required to use the ca_admin certificate.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_6/</guid>
      <description>Unit 6: Issuing a certificate In this unit we will generate different certificates using the web interface and CLI.
The purpose of certificates is to establish trust. Their usage varies depending on the kind of trust they are used to ensure. Some kinds of certificates are used to verify the identity of the presenter; others are used to verify that an object or item has not been tampered with.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_7/</guid>
      <description>Unit 7. Recovering a certificate In this unit we will simulate recovering a user&#39;s private key.
Recover Private Key The URL is different in the images. I&#39;m lazy.
 In new tab, open KRA webpage.
https://ca1.redhat.example.com:8443/kra/agent/kra
 Select PKI Administrator certificate (if prompted).
 Add Exception for insecure connection (if prompted).
 Click Search for Keys.
 Check Key Identifiers box and click Show Key (scroll down).
 Click Details button.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_8/</guid>
      <description>Unit 8: Revoking a certificate In this unit we will revoke a certificate generated during the last unit using the web interface and CLI.
Revoke Certificate (web-based) The URL is different in the images. I&#39;m lazy.
 Open CA webpage.
https://ca1.redhat.example.com:8443/ca/agent/ca
 Select PKI Administrator certificate (if prompted).
 Add Exception for insecure connection (if prompted).
 Click List Certificates.
 Click Find button.
 Copy Serial Number of UID= cert.</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/pki_workshop/unit_lessons/unit_9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/pki_workshop/unit_lessons/unit_9/</guid>
      <description>Unit 9: OCSP Responder In this unit you will interact with the OCSP Responder using the web interface and CLI tool OCSPClient. You will also issue a CRL to verify the revoked certificate is listed there.
Check Certificate Revocation List (Web-based) The URL is different in the images. I&#39;m lazy.
 Open CA webpage.
https://ca1.redhat.example.com:8443/ca/agent/ca/
 Select PKI Administrator certificate (if prompted).
 Add Exception for insecure connection (if prompted).</description>
    </item>
    
    <item>
      <title></title>
      <link>/workshops/secure_software_factory/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/readme/</guid>
      <description>Must have existing OpenShift Environment You can use either of the OpenShift provsioners from RedHatGov - https://github.com/RedHatGov/redhatgov.workshops/tree/master/openshift-aws-setup - https://github.com/RedHatGov/redhatgov.workshops/tree/master/openshift_terraform - https://github.com/gnunn1/openshift-aws-setup - https://github.com/jaredhocutt/openshift-provision - https://github.com/bit4man/ansible_agnostic_deployer
Environment Setup If you&#39;d like to setup an individual environment, use the commands below to set it up or delete the single environment.
Help $ scripts/provision.sh --help
Individual Environment $ scripts/provision.sh deploy --deploy-che --ephemeral
Individual Delete $ scripts/provision.sh delete
Batch Setup If you&#39;d like to setup the workshop for numerous users, go into the provision-batch-setup.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Open source is not only behind many of the technology industry&#39;s most significant innovations, but it is also pervasive across the U.S. government. For more than a decade, Red Hat has been working closely with federal government customers to help them embrace open source technologies.</description>
    </item>
    
    <item>
      <title>CloudForms Workshop Prerequisites</title>
      <link>/workshops/cloudforms41/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/prerequisites/</guid>
      <description>If we have provided a laptop or cloud environment to you for this workshop, the following steps are already set up for you. Please jump to Exercise 1.0  Install git We will use a few git commands directly in some of the advanced labs, please make sure git is installed on your system. You can download the latest version here.
Create GitHub account Create a FREE GitHub account by going to this link and following their step-by-step instructions.</description>
    </item>
    
    <item>
      <title>Conclusion</title>
      <link>/workshops/source_to_image/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/conclusion/</guid>
      <description>Congratulations You have gone through a methodical process to develop container technology that is custom to a framework, yet repeatable. We have:
 Run the application from the command line to understand what it does and how it is supposed to behave.
 Containerized the applicaiton.
 Built Source-to-Image mechanisms that allow us to inject raw source code to be built inside the container.
 Created an OpenShift template to make repeated deployments easier.</description>
    </item>
    
    <item>
      <title>Conclusion - Closing thoughts &amp; homework</title>
      <link>/workshops/security_container_intro/lab99-conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab99-conclusion/</guid>
      <description>As a final homework assignment, we encourage you to watch a video entitled the State of Container Security that Sally and Urvashi presented at an OpenShift Commons briefing on March 27, 2020. Their talk goes into additional detail on many of the concepts covered in this lab along with a sneak peak at future work by the upstream community. They also show some great demos that you will want to see.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contact/</guid>
      <description>We are here to help you Are you curious about something? Do you have some kind of problem with our products?
Please feel free to contact us, our customer service center is working for you 24 hours a day, 7 days a week.
https://www.redhat.com/en/services/support
If you find any issues with the content on this site, please file an issue on our GitHub project at https://github.com/RedHatGov/redhatgov.github.io/</description>
    </item>
    
    <item>
      <title>Deploy Your Own Terminal</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab1.2_terminal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab1.2_terminal/</guid>
      <description>Accessing OpenShift OpenShift provides a web console that allows you to perform various tasks via a web browser.
Let&#39;s Login to the Web Console  Use your browser to navigate to the below URI, and login with the username userYOUR#  and the password provided by your instructor:
 https://console-openshift-console.apps.example.redhatgov.io  Login Webpage
Once logged in you should see your available projects - or a button to create a project if none exist already:</description>
    </item>
    
    <item>
      <title>Exercise 1 - Install JDV and JBDS</title>
      <link>/workshops/jdv_dev/exercise1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise1/</guid>
      <description>Table of Contents  Download JDV Download JBDS Install JDV  Run JDV  Install JBDS  Run JBDS Install teiid designer Configure server settings   Download JBoss Data Virtualization   jboss-dv-6.3.0-1-installer.jar  Download JBoss Developer Studio   devstudio-11.0.0.GA-installer-standalone.jar  Install JBoss Data Virtualization (JDV)   In the location where you downloaded the installer, run the following command
java -jar jboss-dv-6.3.0-1-installer.jar Follow the default prompts with the following exceptions below</description>
    </item>
    
    <item>
      <title>Exercise 1 - The Mount Namespace</title>
      <link>/workshops/containers_the_hard_way/exercise1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise1/</guid>
      <description>Exercise 1.1: Unsharing is Caring New Linux Namespaces are typically spawned by using either the clone or unshare system calls. These exist as C functions but wrappers exist in many other languages. For our purposes today, we will be using the unshare command which is ostensibly a Bash wrapper to the unshare system call.
 unshare --help   Usage: unshare [options] [&amp;lt;program&amp;gt; [&amp;lt;argument&amp;gt;...]] Run a program with some namespaces unshared from the parent.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - A Guided Tour of CloudForms</title>
      <link>/workshops/cloudforms41/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.0/</guid>
      <description>Exercise 1.0: How to Access CloudForms Exercise Description This exercise provides a quick tour of the console to help you get familiar with the user interface, along with some key terminology we will use in subsequent workshop content. If you are already familiar with the basics of CloudForms you can skip this exercise — after making sure you can login.
Key Terms Before sinking our teeth into Cloud Forms, it is important to understand some terminology.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Configuring Ansible Tower</title>
      <link>/workshops/ansible_tower_intro/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_intro/exercise1.0/</guid>
      <description>Exercise Description In this exercise, we are going to configure Red Hat Ansible Tower, so that we can run a playbook.
 There are a number of constructs in the Ansible Tower UI that enable multi-tenancy, notifications, scheduling, etc. However, we are only going to focus on the key concepts required for this workshop today.
   Credentials
  Projects
  Inventory
  Job Template
     Section 1: Logging into Ansible Tower and installing the license key Step 1: Log in To log in, use the username admin and and the password provided by your instructor.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Deploy OpenShift</title>
      <link>/workshops/security_openshift/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.0/</guid>
      <description>OpenShift v3 is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as accurately as possible, with a focus on easy composition of applications by a developer. For example, install Ruby, push code, and add MySQL.
   OpenShift Step 1: Use the oc command
 These instances have been preconfigured with docker and the oc command. The oc command makes deploying OpenShift for development purposes incredibly easy.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Install buildah, skopeo, and podman</title>
      <link>/workshops/containers_101/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/exercise1.0/</guid>
      <description>Exercise 1.0: Installing Buildah, Skopeo and Podman Set up dependencies Elevate privileges
 In order to do any of the steps in the Containers 101 Workshop, you will need root privileges. There are two methods of doing this; the method you choose is up to you. Either method will work, but in a production environment, Method 2 is much more secure and auditable.
 Method 1: The first, and easiest, method is to start a root session, with sudo -i:</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Intro to Cockpit</title>
      <link>/workshops/security_containers/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.0/</guid>
      <description>Exercise 1.0 - Intro to Cockpit   Exercise Description In this workshop, you will learn to navigate Cockpit and examine its features and capabilities.
 What is Cockpit? A remote manager for GNU/Linux servers
 Cockpit is a server manager that makes it easy to administer your GNU/Linux servers via a web browser.
 Cockpit makes it easy for any sysadmin to perform simple tasks, such as administering storage, inspecting journals and starting and stopping services.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Introduction to Security Enhanced Linux (SELinux)</title>
      <link>/workshops/selinux_policy/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise1.0/</guid>
      <description>History In this exercise, we are going to introduce you to Security Enhanced Linux, commonly known as SELinux.
 SELinux is an implementation of the Flask system security architecture. The Flask architecture implements MAC, which focuses on providing an administratively-defined security policy that can control all subjects and objects, basing decisions on all security-relevant information. In addition, Flask focuses on the concept of least privilege, which gives a process exactly the rights it needs to perform its given task.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Running Ad hoc commands</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise1.0/</guid>
      <description>Exercise Description For our first exercise, we are going to run some ad hoc commands to help you get a feel for how Red Hat Ansible Automation works. Ansible ad hoc commands enable you to perform repeatable tasks on local or remote nodes, without having to write a playbook. They are very useful when you simply need to do one or two tasks quickly and often, to many remote nodes.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Running Ad hoc commands</title>
      <link>/workshops/ansible_automation/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise1.0/</guid>
      <description>Exercise Description For our first exercise, we are going to run some ad hoc commands to help you get a feel for how Red Hat Ansible Automation works. Ansible ad hoc commands enable you to perform repeatable tasks on local or remote nodes, without having to write a playbook. They are very useful when you simply need to do one or two tasks quickly and often, to many remote nodes.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Running Ad-hoc commands</title>
      <link>/workshops/ansible_tower_azure/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise1.0/</guid>
      <description>Section 1: Ad hoc commands For our first exercise, we are going to run some ad hoc commands to help you get a feel for how Red Hat Ansible Tower works. Ansible ad hoc commands enable you to perform repeatable tasks on remote nodes, without having to write a playbook. They are very useful when you simply need to do one or two tasks quickly and often, to many remote nodes.</description>
    </item>
    
    <item>
      <title>Exercise 1.0 - Welcome to RHEL!</title>
      <link>/workshops/rhel_8/exercise1.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.0/</guid>
      <description>Exercise Description Welcome to RHEL!
 Red Hat Enterprise Linux 8 is the intelligent enterprise Linux, providing a secure and consistent foundation across hybrid cloud deployments and the tools enterprises need to deliver services and workloads faster with less effort. RHEL 8 enables delivery of any application, on any architecture, on any cloud, physical, or virtual environment, at any time.
 With predictable release cadence and 10-year enterprise support, RHEL 8 is the safe and stable choice for your enterprise.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Creating and Running a Job Template</title>
      <link>/workshops/ansible_tower_intro/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_intro/exercise1.1/</guid>
      <description>Exercise Description This exercise will walk you through the steps required to create a job template and run it. A job template is a definition and set of parameters for running an Ansible job. In other words, a template combines an Ansible project playbook and the settings required to launch it, into one package.
 Templates save setup time, for jobs that are launched repetitively. Once the template is set, it can be edited for future jobs, with different settings.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Explore Your First Infrastructure Provider</title>
      <link>/workshops/cloudforms41/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.1/</guid>
      <description>Exercise 1.1: Working with Infrastructure Providers Exercise Description Learn to view, search and manage infrastructure providers, in Red Hat CloudForms.
In Red Hat CloudForms, an infrastructure provider is a virtual infrastructure environment that you can add to a Red Hat CloudForms appliance, to manage and interact with the resources, in that environment.
In other words, a infrastructure provider is a management platform for managing virtual machines from a single type of hypervisor.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Introduction to SELinux</title>
      <link>/workshops/selinux_policy/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise1.1/</guid>
      <description>Exercise Description To better understand SELinux basic concepts, see the following documentation:
   The SELinux Coloring Book
  SELinux Wiki FAQ
  The SELinux Notebook
   Step 1: Become root First, go ahead and switch users to root:
 sudo -i   Step 2: What mode are we in? Next, let&amp;#8217;s check to see what SELinux mode your host is in:
 getenforce  Enforcing    Step 3: Changing modes Now, we can change the mode that your host is in:</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - OpenShift Templates</title>
      <link>/workshops/security_openshift/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.1/</guid>
      <description>What are OpenShift Templates A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift Container Platform. A template can be processed to create anything you have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Pulling and Running Container Images</title>
      <link>/workshops/containers_101/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/exercise1.1/</guid>
      <description>Exercise 1.1 - Pulling Container Images   Section 1: Pull RHEL Images Step 1. Use the following command to pull a RHEL image from the Red Hat repository: podman pull command podman pull registry.access.redhat.com/rhel7   Images are accessed using the container registry URL (registry.access.redhat.com). If the container registry URL is not specified in the pull command, it could result in the retrieval of an image that originates from an untrusted registry.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Reproducible and Trustworthy Dockerfiles</title>
      <link>/workshops/security_containers/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.1/</guid>
      <description>Exercise 1.1 - Reproducible and Trustworthy Dockerfiles   Exercise Description Now that you&amp;#8217;ve gotten a sense of how Cockpit works, we are going to start digging in at the command line on Container security. You can use either the Terminal in Cockpit from your browser, a SSH terminal or Terminal client like PuTTy. The choice is yours.
   Section 1: Best Practices Step 1.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Running an Ansible playbook</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise1.1/</guid>
      <description>Objective: Configure Edge node to run a Performance CoPilot agent As part of this exercise, you will be running a pre-existing Ansible playbook pre-loaded into your CodeReady Workspace.
 The playbook is written to showcase a series of basic Ansible concepts.
 Note: The following commands below need to be run from within the Ansible terminal
 Step 1: Checking a playbook playbook for correct syntax However, before you run the playbook, lets take a few moments to understand the options.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - What&#39;s New in RHEL 8</title>
      <link>/workshops/rhel_8/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.1/</guid>
      <description>Application streams Application streams deliver user space packages on a cadence that makes sense for the package rather than as a monolithic distribution. Streams also allow the consolidation of previous distribution channels to a single place.
   Red Hat Insights Red Hat Insights is included with your Red Hat® Enterprise Linux® 8 subscription. It helps provide a security-focused, reliable, efficient, and scalable infrastructure environment.
   Image builder Image builder lets customers build images from source RPMs that are deployable to multiple targets quickly and simply, and provides fine-grained control and visibility in to what’s included in a particular image and its history.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Writing Your First Playbook</title>
      <link>/workshops/ansible_automation/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise1.1/</guid>
      <description>Exercise Description Now that you&amp;#8217;ve gotten a sense of how Red Hat Ansible Tower works, we are going to write our first Ansible playbook. The playbook is where you can take some of those ad hoc commands you just ran and put them into a repeatable set of plays and tasks.
 A playbook can have multiple plays and a play can have one or multiple tasks. The goal of a play is to map a group of hosts.</description>
    </item>
    
    <item>
      <title>Exercise 1.1 - Writing Your First playbook</title>
      <link>/workshops/ansible_tower_azure/exercise1.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise1.1/</guid>
      <description>Now that you&amp;#8217;ve gotten a sense of how ansible works, we are going to write our first ansible playbook. The playbook is where you can take some of those ad-hoc commands you just ran and put them into a repeatable set of plays and tasks.
 A playbook can have multiple plays and a play can have one or multiple tasks. The goal of a play is to map a group of hosts.</description>
    </item>
    
    <item>
      <title>Exercise 1.10 - Explore Catalog Configuration</title>
      <link>/workshops/cloudforms41/exercise1.10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.10/</guid>
      <description>Log in to CloudForms using the **admin** account (if not done already).  Exercise 1.10 - Explore Catalog Configuration Section 1: Explore Catalog Configuration Step 1. Select Services → Catalogs.  Services Catalog
Step 2. Select the Catalog Items accordion.  Services Catalog Items
Step 3. Select All Catalog Items → VMware Operations → Deploy Ticket Monster on VMware  Services Catalog Ticket Monster
Step 4. Explore the Details and Selected Resources tabs.</description>
    </item>
    
    <item>
      <title>Exercise 1.11 - Explore Chargeback</title>
      <link>/workshops/cloudforms41/exercise1.11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.11/</guid>
      <description>Exercise 1.11 - Explore Chargeback Exercise Description In Red Hat CloudForms, the chargeback feature of CloudForms Management Engine (CFME) enables you to calculate monetary virtual machine charges, based on owner or company tag. In this exercise you will learn how to review chargebacks, and explore related reports, rates and assignments.
Section 1: Explore Chargebacks CloudForms Management Engine provides a default set of rates for calculating chargeback costs, but you can create your own set of computing and storage costs, as well.</description>
    </item>
    
    <item>
      <title>Exercise 1.12 - Explore Utilization</title>
      <link>/workshops/cloudforms41/exercise1.12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.12/</guid>
      <description>Exercise 1.12 - Explore Utilization Exercise Description CloudForms Management Engine server can collect and analyze capacity and utilization data from your virtual infrastructure. In this exercise, you will learn how to view utilization metrics.
Section 1: Explore Utilization Step 1. Select Optimize → Utilization.  Utilization
Step 2. Select Region 10 → Cloud/Infrastructure Providers → RHV-M.  Region 10 RHV-M Utilization
Step 3. On the Summary tab, review the daily utilization chart.</description>
    </item>
    
    <item>
      <title>Exercise 1.13 - Explore Planning</title>
      <link>/workshops/cloudforms41/exercise1.13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.13/</guid>
      <description># Exercise 1.13 - Explore Planning
## Exercise Description In this exercise you will learn how to plan and optimize a deployment.
Section 1: Explore Planning Step 1. Select Optimize → Planning.  Optimize Planning
Step 2. Under Reference VM Selection, select All VMs.  Reference VM Selection
Step 3. Choose the CFME027 VM. Step 4. Under VM Options, set the Source to Usage. Step 5. Under Target Options/Limits, set Show to Hosts.</description>
    </item>
    
    <item>
      <title>Exercise 1.14 - Examine Policies</title>
      <link>/workshops/cloudforms41/exercise1.14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.14/</guid>
      <description>Exercise 1.14 - Examine Policies Exercise Description Policy profiles are groups of policies that you can apply to entities within CloudForms. Two common types of policies are Compliance policies, which check for a specific state in a host or VM, and Control policies, which control a host or VM, depending on certain criteria.
In this exercise, you will learn how to work with policies, events, conditions, actions, alerts, and profiles.</description>
    </item>
    
    <item>
      <title>Exercise 1.15 - Explore Network Topology</title>
      <link>/workshops/cloudforms41/exercise1.15/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.15/</guid>
      <description>Exercise 1.15 - Explore Network Topology Exercise Description When working in complex provider environments, with many objects, Red Hat CloudForms topology feature can be extremely useful, to quickly view and categorize information. The topology view enables you to view a container provider’s objects, plus their details, such as the properties, status, and relationships to other provider objects. In this exercise, you will learn how to explore the topology of a network.</description>
    </item>
    
    <item>
      <title>Exercise 1.16 - Explore Containers</title>
      <link>/workshops/cloudforms41/exercise1.16/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.16/</guid>
      <description>Exercise 1.16 - Explore Containers Exercise Description In this exercise we will take a general overview of containers and how you will work with them in Red Hat CloudForms.
First, it is important to know the definition of a container: A container is an application sandbox that wraps your software in a complete filesystem, containing everything it needs to run.
 Traditional OS vs. Containers
Exploring Containers Step 1. Go to Compute → Containers.</description>
    </item>
    
    <item>
      <title>Exercise 1.17 - Introduction to the Automate Datastore</title>
      <link>/workshops/cloudforms41/exercise1.17/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.17/</guid>
      <description>Exercise 1.17 - Introduction to the Automate Datastore Exercise Description In this exercise, you will learn about scripting actions, used to control automation datastore functions.
When we use the Automate capability of CloudForms, we write scripts in the Ruby language and use objects that the CloudForms Automation Engine makes available to us. The Automate model is arranged to provide an object oriented hierarchy to control automation functions. The model uses the following organizational units arranged in a hierarchy:</description>
    </item>
    
    <item>
      <title>Exercise 1.18 - “Hello, World!” Automation Script</title>
      <link>/workshops/cloudforms41/exercise1.18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.18/</guid>
      <description>Exercise 1.18 - “Hello, World!” Automation Script Exercise Description Let’s jump right in and start writing our first automation script. In time-honored fashion we’ll write “Hello, World!” to the Automate Engine logfile.
Before we do anything, we need to ensure that the Automation Engine server role is selected on our CloudForms appliance. We do this from the Configure → Configuration menu, selecting the CloudForms server in the Settings accordion.</description>
    </item>
    
    <item>
      <title>Exercise 1.19 - Ansible Inside</title>
      <link>/workshops/cloudforms41/exercise1.19/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.19/</guid>
      <description>Exercise 1.19 - Ansible Inside Exercise Description Red Hat Ansible integration delivers out-of-the-box support for backing service, alert and policy actions using Ansible playbooks. Sync your existing playbook repositories with CloudForms, add credentials to access providers, and create service catalog items for actions ranging from creating and retiring VMs, updating security software, or adding additional disks when space runs low.
Ansible integrates with Red Hat CloudForms to provide automation solutions, using playbooks, for Service, Policy and Alert actions.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Discovering Virtualization Host Systems</title>
      <link>/workshops/cloudforms41/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.2/</guid>
      <description>Exercise 1.2 - Discovering Virtualization Host Systems Exercise Description In this exercise, you will learn how to view, search and manage host systems.
In Red Hat CloudForms, hosts are hypervisors running on physical hardware providing virtual machines and infrastructure.
The CloudForms Management Engine automatically adds hosts from discovered providers. However, you can also discover hosts directly if not using a provider. Discovering hosts is only supported for standalone VMware vSphere servers.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Docker `USER`</title>
      <link>/workshops/security_containers/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.2/</guid>
      <description>Exercise 1.2 - Docker USER   Exercise Description Now that you&amp;#8217;ve gotten a sense of software provenance in Dockerfiles lets take a look at the USER in Dockerfiles.
 By default, containers run as root. A container running as root has full control of the host system. As container technology matures, more secure default options may become available. For now, requiring root is dangerous for others and may not be available in all environments.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Linux Kernel Capabilities</title>
      <link>/workshops/security_openshift/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.2/</guid>
      <description>Before we get into OpenShift lets explore what Linux Kernel Capabilities are and some of the steps that OpenShift has taken to remove certain capabilities by default.
   What are Linux Capabilities? According to the capabilities man page;
 Capabilities are distinct units of privilege that can be independently enabled or disabled.
 Capabilities were added to the kernel around 15 or so years ago to try to divide up the power of root.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Running Your Playbook</title>
      <link>/workshops/ansible_automation/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise1.2/</guid>
      <description>Section 1: Running the playbook We are now going to run your brand-new playbook, on your two web nodes. To do this, you are going to use the ansible-playbook command.
 Step 1: Check your playbook However, before you run the playbook, lets take a few moments to understand the options.
   install_apache.yml This is the name of the playbook that we are running.
  -v Altough not used here, this increases verbosity.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Running Your Playbook</title>
      <link>/workshops/ansible_tower_azure/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise1.2/</guid>
      <description>Section 1: Running the Playbook We are now going to run your brand spankin&#39; new playbook on your two web nodes. To do this, you are going to use the ansible-playbook command.
 Step 1: Run your playbook From your playbook directory ( ~/apache_basic ), run your playbook.
 ansible-playbook -i ../hosts install_apache.yml --private-key=~/.ssh/example-tower   However, before you run that command, lets take a few moments to understand the options.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - SELinux Contexts</title>
      <link>/workshops/selinux_policy/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise1.2/</guid>
      <description>Exercise Description All processes and files are labeled with an SELinux context. The context contains information that SELinux needs to make control decisions. Contexts contain user, role, type, and level information,
 Step 1: Examine a context To get started, let&amp;#8217;s check the context of a file in the ec2-user user&amp;#8217;s home directory:
 ls -Z ~ec2-user/.vimrc  unconfined_u:object_r:user_home_t:s0 /home/ec2-user/.vimrc   As a side note, the -Z flag is present, in a wide variety of common CLI tools, including ls and ps.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Tour of the Web Console</title>
      <link>/workshops/rhel_8/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.2/</guid>
      <description>Exercise Description To get started, we will take a tour of the web console. The web console (called cockpit) will be your primary interactive interface to RHEL. The console has a number of functions that enable you to manage most aspects of the installation.
 Connect to your personal lab environment. Ensure that the information in the footer of the page is correctly filled out and the URL for your environment should show up below</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Working with Container Images</title>
      <link>/workshops/containers_101/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/exercise1.2/</guid>
      <description>Exercise 1.2 - Working with Container Images   Exercise Description In this exercise, you&amp;#8217;ll learn the fundamentals of container images, including how to run them and how to see what is inside of a container. Container storage volumes will be discussed, showing how to mount external file systems, with useful functions and data. We will also discuss container metadata and explain what types of information you can get from them.</description>
    </item>
    
    <item>
      <title>Exercise 1.2 - Writing a playbook to push podman to the edge</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise1.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise1.2/</guid>
      <description>Objective This exercise showcases using Ansible to configure podman and the podman API on a standalone Red Hat Enterprise Linux edge server.
 As part of the execise, you will be exposed to the following elements:
   Ansible modules and supporting parameters (example: &#39;package&#39; and &#39;service&#39; modules)
  How to use loops to make your ansible more efficient and smaller
     Section 1: Creating a directory structure and files for your own playbook For our playbook, we are only going to write one play with two tasks.</description>
    </item>
    
    <item>
      <title>Exercise 1.20 - Ansible Tower</title>
      <link>/workshops/cloudforms41/exercise1.20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.20/</guid>
      <description>Ansible Tower Exercise Description Ansible Tower is a management tool integrated with Red Hat CloudForms, designed to help automate infrastructure operations. Red Hat CloudForms allows you to execute Ansible Tower jobs using service catalogs and Automate. No custom configuration or Ruby scripting is needed in Red Hat CloudForms, as configuration is done in Ansible Tower using playbooks.
You can use the large library of existing Ansible playbooks as Red Hat CloudForms state machines to automate tasks such as backups, package updates, and maintenance in your Red Hat CloudForms environment.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - Discovering VM Systems</title>
      <link>/workshops/cloudforms41/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.3/</guid>
      <description>Exercise 1.3 - Discovering VM Systems Exercise Description In Red Hat CloudForms, infrastructure VMs are operating systems running under a hypervisor. This exercise will enable you to view and manage infrastructure VMs.
Section 1: Explore Infrastructure VMs Step 1: Select Compute → Infrastructure → Virtual Machines.  Infrastructure VMs
Step 2: View the list of your private datacenter’s infrastructure virtual machines. Section 2: Explore the Virtual Thumbnail for Infrastructure VMs The web interface uses virtual thumbnails to represent providers.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - Managing Updates Using the Web Console</title>
      <link>/workshops/rhel_8/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.3/</guid>
      <description>Exercise Description Now that you&amp;#8217;ve toured the web console, lets work through updating the system.
   Section 1: Basic Updating As you did in the previous section, navigate to the Software Updates section of the Web Console, by clicking on the Software Updates link, in the left-hand menu. There will then be a brief interval where Refreshing package information will appear, along with a progress bar. Once that completes, you can proceed with the exercise.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - Remove setuid/setgid Binaries</title>
      <link>/workshops/security_containers/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.3/</guid>
      <description>Exercise 1.3 - Remove setuid/setgid Binaries   Exercise Description This exercise covers permissions that can be set for setuid and setgid binaries and best practices for managing them.
   SETUID/SETGID Overview There are two special permissions that can be set on executable files: Set User ID (setuid) and Set Group ID (sgid). These permissions allow the file being executed to be executed with the privileges of the owner or the group.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - SELinux</title>
      <link>/workshops/security_openshift/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.3/</guid>
      <description>SELinux is primarily a labeling system that assigns a label (name) to every process and system object. This allows every aspect of kernel operations to be first labeled, second classified, and then ultimately enforced by a set of rules that the provider maintains.
 BENEFITS OF RUNNING SELINUX
   All processes and files are labeled with a type. A type defines a domain for processes, and a type for files.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - Using Variables, Loops, and Handlers</title>
      <link>/workshops/ansible_automation/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise1.3/</guid>
      <description>Exercise Description In previous exercises, we showed you the basics of the Ansible core. In the next few exercises, we are going to teach some more advanced Ansible skills that will add flexibility and power to your playbooks.
 Ansible exists to make tasks simple and repeatable. We also know that not all systems are exactly alike and often require some slight customization to the way an Ansible playbook is run.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - Using Variables, Loops, and Handlers</title>
      <link>/workshops/ansible_tower_azure/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise1.3/</guid>
      <description>Exercise Description In previous exercises, we showed you the basics of the Ansible core. In the next few exercises, we are going to teach some more advanced Ansible skills that will add flexibility and power to your playbooks.
 Ansible exists to make tasks simple and repeatable. We also know that not all systems are exactly alike and often require some slight customization to the way an Ansible playbook is run.</description>
    </item>
    
    <item>
      <title>Exercise 1.3 - Working with the Dockerfile</title>
      <link>/workshops/containers_101/exercise1.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/exercise1.3/</guid>
      <description>Exercise 1.3 - Working with the Dockerfile   Exercise Description This exercise will enable you to use a Dockerfile to build a container. Once you understand how images and containers can be created from the command line, you can try building containers in a more permanent way.
 Building container images from Dockerfile files is, by far, the preferred way to create docker-formatted containers, as compared to modifying running containers and committing them to images.</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - CGroups</title>
      <link>/workshops/security_containers/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.4/</guid>
      <description>Exercise 1.4 - CGroups   Exercise Description The Device Cgroup is primary in isolating and limiting containers to their own shares of CPU, memory, disk I/O, network, etc., otherwise the container will be denied. This exercise covers management of memory and CPU limitations.
   Section 1: Memory Limits Container technology uses Cgroups to control and limit the amount of system resources a container can use, and provide administrators with fine-grained control over allocating, prioritizing, denying, managing and monitoring system resources.</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - Configuring Terminal Session Recording</title>
      <link>/workshops/rhel_8/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.4/</guid>
      <description>Section 1. Deploying Session Recording To be able to deploy the Session Recording solution you need to have the following packages installed: tlog, SSSD, cockpit-session-recording.
Note that session recording doesn&amp;#8217;t work for graphical desktop sessions or Cockpit terminal sessions.
 Step 1.1: Pre-Requisites In order to work with session recording, your system will need to have the following installed:
   tlog
  SSSD (System Security Services Deamon)</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - Explore Reports</title>
      <link>/workshops/cloudforms41/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.4/</guid>
      <description>Exercise 1.4 - Explore Reports Exercise Description In this exercise, you will learn how to view, search and customize reports.
In Red Hat CloudForms, reports have been constructed to help you view the most commonly requested and significant data. With appropriate access credentials, you can also create reports. CloudForms Management Engine provides a large group of default reports, organized into categories. Each category has a set of subfolders.
Section 1: Explore Reports Step 1: Select Cloud Intel → Reports.</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - Running the apache-basic-playbook</title>
      <link>/workshops/ansible_automation/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise1.4/</guid>
      <description>Congratulations! You just wrote a playbook that incorporates some key Ansible concepts that you use in most, if not all, of your future playbooks. Before you get too excited though, we should probably make sure it actually runs. So, let&amp;#8217;s do that now.
   Section 1: Running your new apache playbook Step 1: Run your playbook Run your playbook, using the following command.
 cd ~/apache-basic-playbook ansible-playbook site.yml      Section 2: Review If successful, you should see standard output that looks very similar to the following.</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - Running the apache-basic-playbook</title>
      <link>/workshops/ansible_tower_azure/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise1.4/</guid>
      <description>Congratulations! You just wrote a playbook that incorporates some key Ansible concepts that you use in most, if not all, of your future playbooks. Before you get too excited though, we should probably make sure it actually runs. So, let&amp;#8217;s do that now.
   Section 1 - Running your new apache playbook Step 1: Create a host file Make sure you are in the right directory and create a host file.</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - Skopeo</title>
      <link>/workshops/security_openshift/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.4/</guid>
      <description>Skopeo is a command line utility for various operations on container images and image repositories.
 Skopeo is able to inspect a repository on a Docker registry and fetch images layers. By inspect I mean it fetches the repository&amp;#8217;s manifest and it is able to show you a docker inspect-like json output about a whole repository or a tag.
 This tool, in contrast to docker inspect, helps you gather useful information about a repository or a tag before pulling it (using disk space) - e.</description>
    </item>
    
    <item>
      <title>Exercise 1.4 - Working with a Container Registry</title>
      <link>/workshops/containers_101/exercise1.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/exercise1.4/</guid>
      <description>Exercise 1.4 - Working with a Container Registry   Exercise Description In this exercise, you will learn container registry basics, what the registry offers and how to use it.
 What is container registry?
 A basic container registry is a stateless, highly scalable server side application that stores and distributes container images. The docker registry is an open-source project offered under the permissive Apache license.
 Why use container registries?</description>
    </item>
    
    <item>
      <title>Exercise 1.5 - Atomic Scanner</title>
      <link>/workshops/security_openshift/exercise1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.5/</guid>
      <description>The atomic command-line tool provides a way to interact and manage Atomic Host systems and containers. It provides a high level, coherent entrypoint to the system and makes it easier to interact with special kinds of containers, such as super-privileged containers, and debugging tools.
 The atomic command uses tools such as docker, ostree and skopeo to manage containers and container host systems. There are also a lot of features built into the atomic command that are not available in the docker command.</description>
    </item>
    
    <item>
      <title>Exercise 1.5 - Examine Infrastructure Provisioning</title>
      <link>/workshops/cloudforms41/exercise1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.5/</guid>
      <description>Exercise 1.5: Examine Infrastructure Provisioning Exercise Description Learn to provision infrastructures, using Red Hat CloudForms. CloudForms enables provisioning and management across physical, virtual, and private cloud platforms.
Red Hat CloudForms Infrastructure Provisioning process (Summary) In Red Hat CloudForms, when a virtual machine or cloud instance is provisioned, it goes through multiple phases. First, the request must be made. The request includes ownership information, tags, virtual hardware requirements, the operating system, and any customization of the request.</description>
    </item>
    
    <item>
      <title>Exercise 1.5 - Managing Cryptographic Policies</title>
      <link>/workshops/rhel_8/exercise1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.5/</guid>
      <description>Exercise Description RHEL 8 makes it easy to enforce a strong and consistent cryptographic policy on your systems.
 In the following exercises we&amp;#8217;ll use the new commands for managing your crypto configurations and test them. We will be using the web site badssl.com to test connections using different versions of relevant protocols, such as TLS.
   Section 1: Viewing and changing the crypto policy Show the current crypto policy.</description>
    </item>
    
    <item>
      <title>Exercise 1.5 - Namespaces</title>
      <link>/workshops/security_containers/exercise1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.5/</guid>
      <description>Exercise 1.5 - Namespaces   Exercise Description In this exercise, we will examine how to use namespaces to manage initialization processes, in multiple containers.
 PID namespaces enable processes in different containers to have the same PID (process identifier). This means each container can have its own init (PID1) process that manages various system initialization tasks, as well as the container life cycle.
 Also, each container has its unique /proc directory.</description>
    </item>
    
    <item>
      <title>Exercise 1.5 - Roles: Making your playbooks reusable</title>
      <link>/workshops/ansible_automation/exercise1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise1.5/</guid>
      <description>Exercise Description While it is possible to write a playbook in one file, as we&amp;#8217;ve done throughout this workshop, eventually you’ll want to reuse files and start to organize things.
 The &#34;role&#34; feature in Ansible is the way we do this. When you create a role, you deconstruct your playbook into parts and those parts sit in a directory structure. &#34;Wha?? You mean that seemingly useless best practice you mentioned in exercise 1.</description>
    </item>
    
    <item>
      <title>Exercise 1.5 - Roles: Making your playbooks reusable</title>
      <link>/workshops/ansible_tower_azure/exercise1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise1.5/</guid>
      <description>Exercise Description While it is possible to write a playbook in one file as we&amp;#8217;ve done throughout this workshop, eventually you’ll want to reuse files and start to organize things.
 The &#34;role&#34; feature in Ansible is the way we do this. When you create a role, you deconstruct your playbook into parts and those parts sit in a directory structure. &#34;Wha?? You mean that seemingly useless best practice you mentioned in exercise 1.</description>
    </item>
    
    <item>
      <title>Exercise 1.6 - Explore Cloud Providers</title>
      <link>/workshops/cloudforms41/exercise1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.6/</guid>
      <description>Exercise 1.6 - Explore Cloud Providers Exercise Description This exercise will explain how to view cloud providers and related details.
In Red Hat CloudForms, a cloud provider is a computing platform that manages instances and enables the creation of multi-tenant infrastructure services, independently from underlying hypervisors.
Section 1: Explore Cloud Providers Step 1: Select Compute → Clouds → Providers.  Cloud Providers
A list of your private and public cloud providers, such as OpenStack and Amazon EC2, displays.</description>
    </item>
    
    <item>
      <title>Exercise 1.6 - Managing Software in an Application Stream</title>
      <link>/workshops/rhel_8/exercise1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.6/</guid>
      <description>Overview Application streams are new to RHEL 8 and provide access to the latest tools, languages and libraries, without affecting core system components. This capability gives users access to multiple versions of packages, with a known period of support. These application streams, usually provided as modules, can be thought of as package groups that represent an application, a set of tools, or runtime languages.
   Section 1: Stream Availability Step 1: Check to see what PostgreSQL streams are available As root, use yum to see what streams are available, for PostgreSQL:</description>
    </item>
    
    <item>
      <title>Exercise 1.6 - Read Only Containers</title>
      <link>/workshops/security_containers/exercise1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.6/</guid>
      <description>Exercise 1.6 - Read Only Containers   Exercise Description Let&amp;#8217;s move on to &#34;Read Only&#34; Containers. This exercise will examine read-only containers and how to manage them.
 Overview
 Imagine a scenario where an application gets compromised. The first thing the bad guy wants to do is to patch a backdoor into the application, so that the next time the application starts up, it starts up with the backdoor in place.</description>
    </item>
    
    <item>
      <title>Exercise 1.6 - SCC &amp; Seccomp</title>
      <link>/workshops/security_openshift/exercise1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.6/</guid>
      <description>In addition to authorization policies that control what a user can do, OpenShift Container Platform provides security context constraints (SCC) that control the actions that a pod can perform and what it has the ability to access. Administrators can manage SCCs using the CLI. This is a great way to lock down you individual application to make sure that they have hardened settings. This applies to applications running on OpenShift.</description>
    </item>
    
    <item>
      <title>Exercise 1.7 - Explore Cloud Instances</title>
      <link>/workshops/cloudforms41/exercise1.7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.7/</guid>
      <description>#Exercise 1.7 - Explore Cloud Instances
Exercise Description In Red Hat CloudForms, a cloud instance is a virtual machine running under cloud infrastructure. In this exercise, you will learn how to search for cloud instances and review the details related to the cloud instance.
Section 1: Explore Cloud Instances &amp;gt; Select Compute → Clouds → Instances.  Cloud Instances
A list of your public and private cloud instances displays.</description>
    </item>
    
    <item>
      <title>Exercise 1.7 - OpenSCAP Security Compliance Scanning</title>
      <link>/workshops/rhel_8/exercise1.7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.7/</guid>
      <description>Overview RHEL 8 makes it easy to maintain secure and compliant systems with OpenSCAP.  What is SCAP? SCAP (Security Content Automation Protocol) is a NIST project that standardizes the language for describing assessment criteria and findings. It also provides a vulnerability rating system. The project&amp;#8217;s home page is https://scap.nist.gov/
 The essential components of SCAP are:
   XCCDF: The Extensible Configuration Checklist Description Format, to describe security checklists</description>
    </item>
    
    <item>
      <title>Exercise 1.7 - Red Hat Container Catalog</title>
      <link>/workshops/security_openshift/exercise1.7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.7/</guid>
      <description>Red Hat Container Catalog
  While public registries and uncurated repositories are acceptable for some cloud-native development and proof-of-concept projects, they do not always provide content that is fit for production consumption; enterprise workloads require enterprise-ready tools.   MATTHEW HICKSVICE PRESIDENT, ENGINEERING, OPENSHIFT AND MANAGEMENT, RED HAT
 While container-based applications have begun moving into production, not all containers are created or maintained equally. Every container starts with a Linux base layer, which means that every ISV building container images is distributing Linux content.</description>
    </item>
    
    <item>
      <title>Exercise 1.7 - Seccomp</title>
      <link>/workshops/security_containers/exercise1.7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.7/</guid>
      <description>Exercise 1.7 - Seccomp   Exercise Description Secure Computing Mode (seccomp) is a kernel feature that enables you to filter system calls to the kernel from a container. The combination of restricted and allowed calls are arranged in profiles, and you can pass different profiles to different containers. Seccomp provides more fine-grained control than capabilities, giving an attacker a limited number of syscalls from the container. This exercise examines how seccomp works and how it can be employed to provide container security.</description>
    </item>
    
    <item>
      <title>Exercise 1.8 - Explore Group and User Access Controls</title>
      <link>/workshops/cloudforms41/exercise1.8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.8/</guid>
      <description>Exercise 1.8 - Explore Group and User Access Controls Exercise Description This exercise is designed to provide an understanding of Red Hat CloudForms group controls and access controls.
Section 1: Explore Access Control Step 1. Go to Administrator (in the top right corner of the screen) → Configuration and select the Access Control accordion.  Configuration Dashboard
 Access Control
Step 2. Add a role by modifying a copy of an existing role: Step 3.</description>
    </item>
    
    <item>
      <title>Exercise 1.8 - Introduction to using buildah, podman and skopeo to work on containers</title>
      <link>/workshops/rhel_8/exercise1.8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.8/</guid>
      <description>Exercise Description In this exercise, you will build a custom application container utilizing the Red Hat Universal Base Image, customize that image and test the image functionality. After that you will use skopeo to transfer that image from your user local repository to the system repository and configure it to start at boot.
 After completing this section, you will be able to build images from an existing base image using buildah and other host based tools.</description>
    </item>
    
    <item>
      <title>Exercise 1.8 - Red Hat Security API</title>
      <link>/workshops/security_openshift/exercise1.8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.8/</guid>
      <description>Red Hat Product Security is committed to providing tools and security data to help you better understand security threats. This data has been available on our Security Data page and will now also be available in a machine-consumable format with the Security Data API. This tool will allow customers to programmatically query the API for data that was previously exposed only through files on our Security Data page.</description>
    </item>
    
    <item>
      <title>Exercise 1.8 - SELinux</title>
      <link>/workshops/security_containers/exercise1.8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.8/</guid>
      <description>Exercise 1.8 - SELinux   Exercise Description Let&amp;#8217;s move on to Security Enhanced Linux (SELinux). We will cover SELinux basics, Docker SELinux security policy and the &#34;SELinux Coloring Book.&#34;
   Section 1: Basics of SELINUX In this section, we’ll cover the basics of SELinux and containers. SELinux policy prevents a lot of break out situations, where the other security mechanisms fail. With SELinux on Docker, we write policy that says that the container process running as svirt_lxc_net_t can only read/write files with the svirt_sandbox_file_t label.</description>
    </item>
    
    <item>
      <title>Exercise 1.9 - Image Streams &amp; Secrets</title>
      <link>/workshops/security_openshift/exercise1.9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/exercise1.9/</guid>
      <description>Image Stream Image streams can be used to automatically perform an action when new images are created. Builds and deployments can watch an image stream to receive notifications when new images are added and react by performing a build or deployment, respectively.
 For example, if a deployment is using a certain image and a new version of that image is created, a deployment could be automatically performed.</description>
    </item>
    
    <item>
      <title>Exercise 1.9 - Introduction to Catalogs</title>
      <link>/workshops/cloudforms41/exercise1.9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/exercise1.9/</guid>
      <description>The following steps are initially run as a non-admin customer account.  Exercise 1.9 - Introduction to Catalogs Exercise Description This exercise enables you to explore and verify the catalog configuration.
Section 1: Explore Catalog Configuration Step 1. Select Services → Catalogs.  Services Catalog
Step 2. On the right, select the Deploy Ticket Monster on VMware catalog item.  Ticket Monster
Step 3. Click Order to order the service.</description>
    </item>
    
    <item>
      <title>Exercise 1.9 - Linux Kernel Capabilities</title>
      <link>/workshops/security_containers/exercise1.9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/exercise1.9/</guid>
      <description>Exercise 1.9 - Linux Kernel Capabilities   Exercise Description In this exercise, we will examine Linux capabilities: how they are defined, their purpose,and how they affect security.
 Linux Capabilities Defined
 According to the capabilities man page;
 Capabilities are distinct units of privilege that can be independently enabled or disabled.
 Capabilities were added to the kernel around 15 or so years ago, to try to divide up the power of root.</description>
    </item>
    
    <item>
      <title>Exercise 1.9 - System Roles with Ansible</title>
      <link>/workshops/rhel_8/exercise1.9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/exercise1.9/</guid>
      <description>Exercise Description RHEL 7.4 introduced RHEL System Roles, a collection of Ansible roles and modules that provide a stable and consistent configuration interface for remote management of Red Hat Enterprise Linux. These System Roles are applicable to RHEL 6.10 and above, easing management of 3 generations of RHEL. The effort is based on development of the Linux System Roles project (link: https://linux-system-roles.github.io/upstream). The following roles are provided and supported as follows:</description>
    </item>
    
    <item>
      <title>Exercise 10 - View data in AngularJS client</title>
      <link>/workshops/jdv_dev/exercise10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise10/</guid>
      <description>View the data services  Go back to the Angualar client at http://localhost:8181
 View each of the data services we created by navigating the lefthand pane, including the combined data service for all 3 sources
  Change users to view masking/filtering  In the upper right, click the user icon and change the user to maskUser
 Go to the Federated page or click reload if already on the page.</description>
    </item>
    
    <item>
      <title>Exercise 2 - PID and IPC Namespaces</title>
      <link>/workshops/containers_the_hard_way/exercise2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise2/</guid>
      <description>Exercise 2.1 - The PID Basics Before we start playing with PID namespaces, it is good to have some background on how PIDs in Linux work.
 Every process that runs on a Linux system gets a Process ID (PID). This exists as a 32-bit integer. Numbering starts at 1 and increments until there are none left. PIDs can be reused once the process terminates. If you run out of PIDs, you&amp;#8217;ll have a bad time.</description>
    </item>
    
    <item>
      <title>Exercise 2 - Prepare the datasources</title>
      <link>/workshops/jdv_dev/exercise2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise2/</guid>
      <description>Get the course repositories  There are two repositories, one that contains a sample JDV project with helper data/scripts and another which is the AngularJS frontend.
 Pull down the JDV project
git clone https://github.com/mechevarria/jdv-demo Next pull down the AngularJS client
https://github.com/mechevarria/jdv-demo-client  Add the workshop data to PostgreSQL  The script that needs to be run is  jdv-demo/assets/postgres/create_insert.sql
 If you are running the instance from TurnKey Linux, you can load the data via a browser.</description>
    </item>
    
    <item>
      <title>Exercise 2.0 - Build a CI/CD pipeline and container in Red Hat OpenShift</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise2.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise2.0/</guid>
      <description>Exercise Description As part of the Applied Ansible workshop, you will deploy (copy/transfer) a container built on top of Red Hat OpenShift to an edge node running only Red Hat Enterprise Linux utilizing Ansible Automation. Prior to that step, we need to actually &#34;create&#34; a container on Red Hat OpenShift.
   Let&amp;#8217;s Login to the Web Console In your browser &amp;amp; in a new tab, connect to the Red Hat OpenShift console</description>
    </item>
    
    <item>
      <title>Exercise 2.0 - Creating Custom SELinux Policy</title>
      <link>/workshops/selinux_policy/exercise2.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise2.0/</guid>
      <description>Exercise Description In this exercise, we are going to download the code for the example application that we are going to write policy for, and build and install it onto our test system.
 Step 1: Change directories Create a src directory, in your home directory.
 cd ~ mkdir src cd src   Step 2: Check out the source code to the example application from GitHub Download the latest code release</description>
    </item>
    
    <item>
      <title>Exercise 2.0 - Installing Ansible Tower</title>
      <link>/workshops/ansible_automation/exercise2.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise2.0/</guid>
      <description>Exercise Description In this exercise, we are going to get Ansible Tower installed on your control node.
 Step 1: Change directories Change directories to /tmp
 cd /tmp    Step 2: Download Red Hat Ansible Tower Download the latest Ansible Tower package
 curl -O https://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-3.7.3-1.tar.gz    Step 3: Untar and unzip the package file tar xvfz /tmp/ansible-tower-setup-3.7.3-1.tar.gz    Step 4: Change directories Change directories into the Ansible Tower setup package</description>
    </item>
    
    <item>
      <title>Exercise 2.0 - Installing Ansible Tower</title>
      <link>/workshops/ansible_tower_azure/exercise2.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise2.0/</guid>
      <description>Exercise Description In this exercise, we are going to get Ansible Tower installed on your control node
   Installing Ansible Tower Step 1: Change directories Change directories to /tmp
 cd /tmp    Step 2: Download Red Hat Ansible Tower Download the latest Ansible Tower package
 curl -O http://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-latest.tar.gz    Step 3: Untar and unzip the package file tar xvfz /tmp/ansible-tower-setup-latest.tar.gz    Step 4: Change directories Change directories into the Ansible Tower package</description>
    </item>
    
    <item>
      <title>Exercise 2.1 - Configuring Ansible Tower</title>
      <link>/workshops/ansible_automation/exercise2.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise2.1/</guid>
      <description>Exercise Description In this exercise, we are going to configure Red Hat Ansible Tower, so that we can run a playbook.
 There are a number of constructs in the Ansible Tower UI that enable multi-tenancy, notifications, scheduling, etc. However, we are only going to focus on the key concepts required for this workshop today.
   Credentials
  Projects
  Inventory
  Job Template
     Section 1: Logging into Ansible Tower and installing the license key Step 1: Log in To log in, use the username admin and and the password ansibleWS.</description>
    </item>
    
    <item>
      <title>Exercise 2.1 - Configuring Ansible Tower</title>
      <link>/workshops/ansible_tower_azure/exercise2.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise2.1/</guid>
      <description>Exercise Description In this exercise, we are going to configure Tower so that we can run a playbook.
     There are a number of constructs in the Ansible Tower UI that enable multi-tenancy, notifications, scheduling, etc. However, we are only going to focus on a few of the key concepts required for this workshop today.       Credentials
  Projects
  Inventory</description>
    </item>
    
    <item>
      <title>Exercise 2.1 - Generating a generic policy</title>
      <link>/workshops/selinux_policy/exercise2.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise2.1/</guid>
      <description>Exercise Description In this exercise, we are going to automatically generate an SELinux policy framework, and learn to handle AVC messages that are generated by SELinux, when the application violates the policy.
 Step 1: Generate an initial generic SELinux policy Create a policy directory, and generate an initial policy. Use sepolicy generate to generate a policy for the app that we want to enable.
 cd ~/src mkdir policy cd policy sepolicy generate --init /usr/local/sbin/testapp  Note the last few lines in the output from sepolicy generate:</description>
    </item>
    
    <item>
      <title>Exercise 2.1 - Run CI/CD pipeline (powered by Tekton) in Red Hat OpenShift</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise2.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise2.1/</guid>
      <description>Exercise Description View the created pipeline and review the container that Ansible orchestrate deploying to the edge node.
 Let&amp;#8217;s review the container build Select Pipelines in the left menu:
    === Click on Pipeline Runs in the menu, and then edge-?????? (where ?????? is some alphanumeric sequence):
     Wait until pipeline details are green. Feel gree to watch the progress by selecting the Logs tab</description>
    </item>
    
    <item>
      <title>Exercise 2.2 - Creating a Custom SELinux Application Policy</title>
      <link>/workshops/selinux_policy/exercise2.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise2.2/</guid>
      <description>Exercise Description This exercise will walk you through the steps required to handle AVC messages that are generated by SELinux, when an application violates existing policy. We will modify the already-created generic policy, in order to allow the application to run correctly, when we switch the policy to enforcing mode.
 Step 1: Check for AVC denials Now that our app is up and running, we can check the system logs for AVC (access vector cache, where SELinux caches decisions to grant or deny access) denial messages.</description>
    </item>
    
    <item>
      <title>Exercise 2.2 - Creating and Running a Job Template</title>
      <link>/workshops/ansible_automation/exercise2.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise2.2/</guid>
      <description>Exercise Description This exercise will walk you through the steps required to create a job template and run it. A job template is a definition and set of parameters for running an Ansible job. In other words, a template combines an Ansible project playbook and the settings required to launch it, into one package.
 Templates save setup time, for jobs that are launched repetitively. Once the template is set, it can be edited for future jobs, with different settings.</description>
    </item>
    
    <item>
      <title>Exercise 2.2 - Creating and Running a Job Template</title>
      <link>/workshops/ansible_tower_azure/exercise2.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise2.2/</guid>
      <description>Exercise Description This exercise will walk you through the steps required to create a job template and run it.
 A job template is a definition and set of parameters for running an Ansible job. In other words, a template combines an Ansible project playbook and the settings required to launch it, into one package. Templates save setup time, for jobs that are launched repetitively. Once the template is set, it can be edited for future jobs, with different settings.</description>
    </item>
    
    <item>
      <title>Exercise 2.3 - Creating Policy Rules for Network Access</title>
      <link>/workshops/selinux_policy/exercise2.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise2.3/</guid>
      <description>Exercise Description In this exercise, we will continue the modification of the generic application polocy to allow network traffic to be generated by the test application. There are several rules that will be needed, to handle a variety of AVC denials.
   Section 1: HTTP Port Connectivity Step 1: Check for AVC denials Let&amp;#8217;s restart our app, to get an updated list of denials.
 sudo systemctl restart testapp  Let&amp;#8217;s start with one of the denials that will be present in the audit search results, related to connecting on TCP port 80:</description>
    </item>
    
    <item>
      <title>Exercise 2.4 - Finishing the Policy</title>
      <link>/workshops/selinux_policy/exercise2.4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/exercise2.4/</guid>
      <description>Exercise Description In this final exercise, we will finish the testapp SELinux policy.
   Section 1: A Last-ish Interface Step 1: Check for AVC denials Let&amp;#8217;s again restart our app, to get an updated list of denials.
 systemctl restart testapp  We only have a few denials left, and you will see that they all (probably!) reference /etc/resolv.conf or to /etc/hosts.
 ausearch -m AVC -ts recent | egrep &amp;#39;^type=AVC&amp;#39;  type=AVC msg=audit(1553195947.</description>
    </item>
    
    <item>
      <title>Exercise 3 - Network and UTS Namespaces</title>
      <link>/workshops/containers_the_hard_way/exercise3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise3/</guid>
      <description>Exercise 3.1 - Network Stack Background The networking namespace in the Kernel is actually responsible for a surprisingly large number of things. It keeps tabs on all the network interfaces and their state, routing tables (Did you know there is more than one? By default, there are 4.), firewall rules, etc. All of these things are explored using the ip command. This command allows root to view and modify the networking stack.</description>
    </item>
    
    <item>
      <title>Exercise 3 - Prepare Postman and AngularJS client</title>
      <link>/workshops/jdv_dev/exercise3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise3/</guid>
      <description>Prepare the AngularJS client  In the jdv-demo-client folder run the following command
npm install Afterwards run
npm run dev To start a node webserver that will host the AngularJS application.
  * To check go to http://localhost:8181 in your browser. You will get an error when checking the pages. That is ok since we are going to be building the data services for the web application.
Prepare Postman  Import the REST tests into Postman by going to file  import Select jdv-demo/assets/demo-jdv.</description>
    </item>
    
    <item>
      <title>Exercise 3.0 - Configure Red Hat Enterprise Linux to run containers</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise3.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise3.0/</guid>
      <description>Exercise Description As part of the exercise, we are going to run a login command on the edge node to set a temporary token. This token will allow the edge node to log into the Red Hat OpenShift registry and download containers.
 As part of the exercise, we have built a playbook for you; however, no all of the variables are assigned. This allows us to distribute the playbook to multiple workshops and you the capability to understand how to run playbooks in Ansible tower and assign custom variables to those plays.</description>
    </item>
    
    <item>
      <title>Exercise 3.0 - Using Ansible to Implement Security</title>
      <link>/workshops/ansible_automation/exercise3.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/exercise3.0/</guid>
      <description>Exercise Description In this exercise, we are going to use Red Hat Ansible Tower to run a Operating System Protection Profile (OSPP) for RHEL 8.x for our environment
 Step 1: Generate a security profile using OpenSCAP. In your Web Console Terminal window (if you closed it, see the SETUP step, in your workbook), install OpenScap on the Tower node:
 ansible localhost -m package -a &#34;name=scap-security-guide state=present&#34; -b   Next, setup a local projects directory to stage a remediation playbook</description>
    </item>
    
    <item>
      <title>Exercise 3.0 - Using Ansible to Implement Security</title>
      <link>/workshops/ansible_tower_azure/exercise3.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/exercise3.0/</guid>
      <description>In this exercise, we are going to use Ansible Tower to run DISA STIG and NIST 800-53 evaluations of our environment. Note that the NIST 800-53 role also includes the execution of DISA STIG evaluation against targeted hosts.
   DISA STIG controls https://galaxy.ansible.com/MindPointGroup/RHEL7-STIG/
  NIST 800-53 controls https://galaxy.ansible.com/rhtps/800-53/
     Adding the DISA STIG and NIST 800-53 role to your Tower node Step 1: Download role to Ansible roles directory In your wetty window (if you closed it, see the SETUP step, in your workbook), type the following:</description>
    </item>
    
    <item>
      <title>Exercise 3.1 - Setup Ansible template to stage temporary access token on edge node</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise3.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise3.1/</guid>
      <description>Exercise 3.1 - Setup a template to load a temporary token on the edge node Let&amp;#8217;s bring Projects, Inventories, and Crentials all together with &amp;#8230;&amp;#8203; Templates Select Templates
  Templates tab - Left hand side of screen  Select the + sign to add a new template; then select Job Template
  Add Job Template      NAME
 Setup edge authentication to registry - user1</description>
    </item>
    
    <item>
      <title>Exercise 3.2 - Running an Ansible Automation template</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise3.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise3.2/</guid>
      <description>Exercise 3.2 - Retrieve user token from OpenShift and launch login template In a new tab, log into Red Hat OpenShift  https://console-openshift-console.apps.example.redhatgov.io 
    Navigate to the user1 menu in the upper right hand corner and select Copy login command.
 You will be prompted to login again. Once complete, select Display Token
    Highlight and copy the API token, it will look like sha256~ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopq</description>
    </item>
    
    <item>
      <title>Exercise 3.3 - Create new template to stage containers on edge node</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise3.3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise3.3/</guid>
      <description>Exercise 3.3 - Configure podman, systemd, and pull a container from OpenShift Configure new template to field containers to edge node Select Templates
  Templates tab - Left hand side of screen  Select the + sign to add a new template; then select Job Template
  Add Job Template      NAME
 Field container to edge node - user1
   DESCRIPTION</description>
    </item>
    
    <item>
      <title>Exercise 4 - Create new teiid project</title>
      <link>/workshops/jdv_dev/exercise4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise4/</guid>
      <description>Create a new JDV project  In JBDS, do window  perspective  open perspective  other  teiid designer
 file  new  teiid model project
 Name the project jdv-demo and its ok to leave the default location.
 Do change the default folders to sources, views and federated
   [teiid](http://teiid.jboss.org/) is the the upstream community project for JBoss Data Virtualization  Create a VDB  file  New  Teiid VDB</description>
    </item>
    
    <item>
      <title>Exercise 4 - User Namespace</title>
      <link>/workshops/containers_the_hard_way/exercise4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise4/</guid>
      <description>Exercise 4.1 - History Lesson The User Namespace is easily the newest namespce in the Linux Kernel. It was introduced to allow non-root users to run containers without a heavy root daemon. Typically, a non-priviledged user cannot spawn new namespaces. However, if a new User Namespace is created, then that same user can now spawn any type of namespace. Since this feature was introduced in the Kernel, you have seen newer container images come out that do not rely on daemons because they are no longer necessary.</description>
    </item>
    
    <item>
      <title>Exercise 4.0 - Update to a new container on OpenShift and field to edge</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/exercise4.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/exercise4.0/</guid>
      <description>Exercise Description Now that we can pull a container from OpenShift and move it to an edge node via Ansible Automation, let&amp;#8217;s repeat the process with a new release using the artifacts that we created.
   Let&amp;#8217;s Login to the Web Console In your browser &amp;amp; in a new tab, connect to the Red Hat OpenShift console
  https://console-openshift-console.apps.example.redhatgov.io 
    Let&amp;#8217;s build the pipeline with a new release Select Pipelines in the left menu</description>
    </item>
    
    <item>
      <title>Exercise 5 - All Together Now</title>
      <link>/workshops/containers_the_hard_way/exercise5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise5/</guid>
      <description>Let&amp;#8217;s put all of our new namespace skills to use! Again, we will be working as the rhel user.
 su - rhel # if neccessary unshare -mipfnuUr /bin/bash mount -t proc none /proc mount -t tmpfs none /tmp mount -t sysfs none /sys   With the call to unshare, we used all of the flags that we have used in the previous exercises. We also mounted three new filesystems after creating our sandbox.</description>
    </item>
    
    <item>
      <title>Exercise 5 - Import PostgreSQL data</title>
      <link>/workshops/jdv_dev/exercise5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise5/</guid>
      <description>Import JDBC datasource  file  import  teiid designer  JDBC Database &amp;gt;&amp;gt; source model   Under Connection Profile, select new
 Select PostgreSQL as the Connection Profile Type, Next
 In the next window, click the New Driver Definition icon
 Under the Jar List tab, select Add JAR/Zip and then select jdv-demo/assets/postgres/postgresql-42.1.4.jar, then ok
 In the New JDBC Connection Profile window make sure to use the IP address of your PostgreSQL instance and put the credentails for the postgres user and check Save Password.</description>
    </item>
    
    <item>
      <title>Exercise 6 - Changing the Filesystem</title>
      <link>/workshops/containers_the_hard_way/exercise6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise6/</guid>
      <description>Exercise 6.1 - Some Background All of that is well and good. However, containers are great because they allow us to package and distribute software. All we&amp;#8217;ve done is isolate some Linux processes. Where&amp;#8217;s the beef?
 Containers get their root filesystem from a container image file. This is typically pulled from a registry. This file is, essentially, a TAR archive with the root filesystem contents and some metadata. Those contents are then made available to the running container through use of an Overlay Filesystem.</description>
    </item>
    
    <item>
      <title>Exercise 6 - Import REST data</title>
      <link>/workshops/jdv_dev/exercise6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise6/</guid>
      <description>Import REST source  file  import  Web Service Source &amp;gt;&amp;gt; Source and View Model (REST)
 Click New Connection Profile, then next
 Connection URL https://jsonplaceholder.typicode.com/users
 Response type JSON
 Make sure Test Connection returns Ping Succeeded!, then Finish
   Click Next
 Set the following values like the screenshot below
   Next and put JNDI as java:/RestDS, then Next
 In the Import from REST Web Service Source, expand the first response element then right click on the second response element and select Set as Root path</description>
    </item>
    
    <item>
      <title>Exercise 7 - Container Networking</title>
      <link>/workshops/containers_the_hard_way/exercise7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/exercise7/</guid>
      <description>Exercise 7.1 - Some Background The Linux Kernel provides many useful primitives that can be used to construct very intricate network configurations. They are very flexible and powerful. Of particular interest today are Virtual Ethernet Adapaters, Bridges, and Taps.
 A Virtual Ethernet Adapater operates in the Kernel exactly the way a physical NIC would. The only difference is that instead of receiving data from a physical device, a virtual adapter recieves data from another network adapter in the stack.</description>
    </item>
    
    <item>
      <title>Exercise 7 - Import Excel data</title>
      <link>/workshops/jdv_dev/exercise7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise7/</guid>
      <description>Verify Settings  You need to first verify that the Excel extensions were imported correctly into the designer
 window  show view  other 
 Teiid Designer  Model Extension Registry
   In the Model Extension Registry tab at the bottom, verify in the Namespace Prefix you see the value excel and a checkbox in the imported column. If not, close the designer, re-open, click the start button to mark the server as started, and check again.</description>
    </item>
    
    <item>
      <title>Exercise 8 - Federate Data</title>
      <link>/workshops/jdv_dev/exercise8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise8/</guid>
      <description>Create the combined view file -&amp;gt; new -&amp;gt; teiid metadata model
 Location demo-jdv/federated
 Model Name FedView
 Model Type View Model
 Do not select any options under model builder and click Finish
 Right-click on the FedView Package diagram and select new -&amp;gt; child -&amp;gt; table
 Select Option 1: Build with new table wizard
 Name combined
  Under the PK tab, check the include box then put id_pk in the name field.</description>
    </item>
    
    <item>
      <title>Exercise 9 - Secure Data Services</title>
      <link>/workshops/jdv_dev/exercise9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/exercise9/</guid>
      <description>Overview  To secure the services we created in our VDB, we need to create an additional user on the server.  Create the additional user  In $JBOSS_HOME/bin directory run the following script with the answers to the prompts below
./add-user.sh   For windows users, the script will be **add-user.bat**   Type: b) Application User Username: maskUser Password: Password1! (same as the one used during installation groups: odata,mask Remoting: no</description>
    </item>
    
    <item>
      <title>Fin</title>
      <link>/workshops/secure_software_factory/fin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/fin/</guid>
      <description>That&#39;s it! Hopefully, you understand how to build a Trusted Software Supply Chain on OpenShift.</description>
    </item>
    
    <item>
      <title>Fin.</title>
      <link>/workshops/openshift_101_dcmetromap/fin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/fin/</guid>
      <description>That&#39;s it! Hopefully, these labs provided you some idea of how to perform common tasks within the OpenShift environment. And hopefully, you have a deeper understanding of how containers and container orchestration works. Please feel free to continue to &amp;quot;kick the tires&amp;quot; in the demo environment we&#39;ve setup and explore both the web console and the oc command line client.
Get even deeper While you&#39;re here, we can help you install the Container Development Kit (CDK) on your laptop.</description>
    </item>
    
    <item>
      <title>Fin.</title>
      <link>/workshops/openshift_4_101/fin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/fin/</guid>
      <description>That&#39;s it! Hopefully, these labs provided you some idea of how to perform common tasks within the OpenShift environment. And hopefully, you have a deeper understanding of how containers and container orchestration works. Please feel free to continue to &amp;quot;kick the tires&amp;quot; in the demo environment we&#39;ve setup and explore both the web console and the oc command line client.
 OpenShift Architecture OpenShift Developer&#39;s Site   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.</description>
    </item>
    
    <item>
      <title>Fin.</title>
      <link>/workshops/openshift_4_101_dynatrace/fin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/fin/</guid>
      <description>That&#39;s it! Hopefully, these labs provided you some idea of how to perform common tasks within the OpenShift environment. And hopefully, you have a deeper understanding of how containers and container orchestration works. Please feel free to continue to &amp;quot;kick the tires&amp;quot; in the demo environment we&#39;ve setup and explore both the web console and the oc command line client.
 OpenShift Architecture OpenShift Developer&#39;s Site   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.</description>
    </item>
    
    <item>
      <title>Fin.</title>
      <link>/workshops/openshift_service_mesh_v1.0/fin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/fin/</guid>
      <description>That&#39;s it! Hopefully, these labs provided you some idea of why a service mesh is critical when developing and operating microservice applications. Istio is an emerging technology so keep checking the upstream announcements and blog for exciting news.
Also, release notes are a great place to see changes that occurred between releases. You can find the OpenShift Service Mesh release notes here and the upstream Istio project release notes here.</description>
    </item>
    
    <item>
      <title>Fin.</title>
      <link>/workshops/rhosp_101/fin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/fin/</guid>
      <description>That&#39;s it! Hopefully, these labs provided you some idea of how to perform common tasks within the OpenStack environment. And hopefully, you have a deeper understanding of how Infrastructure-as-a-Service works.
Get even deeper Here are some good resources to continue learning OpenStack:
 OpenStack Foundation RDO Project - Our midstream, open source OpenStack distrobution for RHEL variants Red Hat OpenStack Platform Official Docs  There are two ways to get started with RDO: TripleO Quickstart and PackStack</description>
    </item>
    
    <item>
      <title>Intro to Service Mesh - Adding a New Service</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab2.2_deployuserprofile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab2.2_deployuserprofile/</guid>
      <description>Adding a New Service to the Mesh You need to deploy the new user profile application into the service mesh.
Deploy Application The deployment file &#39;userprofile-deploy-all.yaml&#39; was created for you to deploy the application. The file creates the user profile service and an accompanying PostgreSQL database. Similar to the other source files, an annotation &#39;sidecar.istio.io/inject&#39; was added to tell Istio to inject a sidecar proxy and add this to the mesh.</description>
    </item>
    
    <item>
      <title>Intro to Service Mesh - Building a Microservice</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab2.1_userprofile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab2.1_userprofile/</guid>
      <description>Building a Microservice   In the browser, navigate to the &#39;Profile&#39; section in the header.   If you lost the URL, you can retrieve it via:
echo $GATEWAY_URL

You should see the following:
 Unknown Profile Page
The UI shows an unknown user and that&#39;s because there&#39;s no profile service for your application. You are going to build a new microservice for user profiles and add this to your service mesh.</description>
    </item>
    
    <item>
      <title>Intro to Service Mesh - Deploying an App</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab1.4_deploymsa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab1.4_deploymsa/</guid>
      <description>Deploying an App into the Service Mesh It&#39;s time to deploy your microservices application. The application you are working on is a paste board application in which users can post comments in shared boards. Here is a diagram of the architecture:

App Architecture
The microservices include single sign-on (SSO), user interface (UI), the boards application, and the context scraper. In this scenario, you are going to deploy these services and then add a new user profile service.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/workshops/containers_the_hard_way/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/intro/</guid>
      <description>What is a container? It is a simple question without a simple answer. Search the internet for this question and you&amp;#8217;ll find an endless stream of blog posts explaining how containers differ from virtual machines. That&amp;#8217;s great, but that doesn&amp;#8217;t answer my question. What is a container? Does anybody really understand what is happening on their server what they podman run?
   By the end of this workshop, you will feel confident answering this question and explaining it to your peers.</description>
    </item>
    
    <item>
      <title>Introduction to Red Hat OpenStack Platform</title>
      <link>/workshops/rhosp_101/1_introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/1_introduction/</guid>
      <description>Red Hat OpenStack Platform Overview Red Hat OpenStack Platform is implemented as a collection of interacting services that control compute, storage, and networking resources. The following diagram provides a high-level overview of the OpenStack core services.
  OpenStack Services Overview    Horizon - User interface built to give a graphical way to interact with OpenStack services Keystone - Identity management service for projects, users, groups, roles, endpoints, etc.</description>
    </item>
    
    <item>
      <title>Key Terms</title>
      <link>/workshops/openshift_4_101/keyterms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/keyterms/</guid>
      <description>Key Terms We will be using the following terms throughout the workshop labs; so, here are some basic definitions you should be familiar with. You&#39;ll learn more terms along the way; but, these are the basics to get you started.
 Container - Your software wrapped in a complete filesystem containing everything it needs to run Image - We are talking about docker images; read-only and used to create containers Image Stream - An image stream comprises one or more OCI images identified by tags.</description>
    </item>
    
    <item>
      <title>Lab - Building and Deploying a Fast-Moving Monolith</title>
      <link>/workshops/strangling_the_monolith/lab1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/strangling_the_monolith/lab1/</guid>
      <description>FAST-MOVING MONOLITH  Large organizations have a tremendous amount of resources invested in existing monolith applications Looking for a sane way to capture the benefits of containers and orchestration without having to complete rewrite OpenShift provides the platform for their existing investment with the benefit of a path forward for microservice based apps in the future  FAST-MOVING MONOLITH ADVANTAGES  Easier to develop since all dependencies are included Single code base for teams to work on No API backwards compatibility issues since all logic is packaged with the application Single deployable unit  Step 1  In this lab, the coolstore monolith will be built and deployed to OpenShift from your local workstation demonstrating a typical Java application developer workflow A sample pipeline is included which will be used to deploy across dev and prod environment   First, deploy the coolstore monolith dev project (don’t forget to include -b app-partner when you run git clone - this is the branch in use for this lab!</description>
    </item>
    
    <item>
      <title>Lab - Containerize the App</title>
      <link>/workshops/source_to_image/containerize_app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/containerize_app/</guid>
      <description>Introduction Now we want to containerize the Gochat app. If you are using the CLI on your own computer, you could use the docker command. However, if you are in the Wetty terminal, remember you are actually in a container. We could have exposed the container socket on the host to your Wetty container. However, this would break the security model.
Fortunately we can build from a Dockerfile using the oc command line tool.</description>
    </item>
    
    <item>
      <title>Lab - Gochat S2I</title>
      <link>/workshops/source_to_image/gochat_s2i/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/gochat_s2i/</guid>
      <description>Step 1 - Create a New Project Space In the Wetty terminal, create a new project.
oc new-project gochat-s2i-userYOUR#  Step 2 - Create the Golang S2I Builder Image Create a new build for the Golang S2I builder image
cd ~oc new-build golang-s2i/ --to=golang-s2i Start the new build for the Golang S2I builder image
oc start-build golang-s2i --from-dir=golang-s2i/ Step 3 - Import YAML Template in to OpenShift In the OpenShift WebUI, on the catalog page, seclect Import YAML/JSON Import YAML/JSON        Step 4 - Configure the Import Select the project to which you want to import the template</description>
    </item>
    
    <item>
      <title>Lab - Login &amp; Tour of OpenShift</title>
      <link>/workshops/strangling_the_monolith/login_tour_oc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/strangling_the_monolith/login_tour_oc/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the console to help you get familiar with the user interface along with some key terminology we will use in subsequent lab content. If you are already familiar with the basics of OpenShift you can skip this lab - after making sure you can login.
Wetty Environment Links Your instructor will assign you an OpenShift environment and login.
Please click &amp;quot;Return to Workshop&amp;quot; above to view the OpenShift Environment Links.</description>
    </item>
    
    <item>
      <title>Lab - Login &amp; Tour of Wetty</title>
      <link>/workshops/strangling_the_monolith/login_tour_wetty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/strangling_the_monolith/login_tour_wetty/</guid>
      <description>Introduction to Wetty (Browser-based SSH) This lab provides a quick tour of the browser based SSH client Wetty. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
Accessing Wetty Use this URL to access the Wetty node, just change the workshopname. Ask your instructor for rthe workshopname.
https://&amp;lt;workshopname&amp;gt;.wetty.redhatgov.io:8888 After logging in, you should see a shell.
The wetty instance will already have the &#39;oc&#39; command installed on them.</description>
    </item>
    
    <item>
      <title>Lab - Microservice Integration Patterns</title>
      <link>/workshops/strangling_the_monolith/lab3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/strangling_the_monolith/lab3/</guid>
      <description>Microservice Integration Patterns  In previous labs, we created two new microservices with the intention of replacing functionality (“strangling”) the monolithic application. Currently no traffic is routed to them. If you were to re-route traffic from the monolith’s /services/products API to the new catalog service’s /services/catalog endpoint, you would be missing the inventory data. In this lab we will consider different options and architectures for integrating the microservices’ functionality into our app.</description>
    </item>
    
    <item>
      <title>Lab - Roll Dice</title>
      <link>/workshops/source_to_image/dice/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/dice/</guid>
      <description>In an attempt at nostalgia, we are going to once again create an S2I builder, this time for a Fortran app. Yes, Fortran. The objective is to make our little chat program a bit more interesting.
Step 1 - Create the S2I Project cd ~s2i create fortran-s2i fortran-s2i Step 2 - Edit the Dockerfile cd ~cat /dev/null &amp;gt; ~/fortran-s2i/Dockerfilevi ~/fortran-s2i/Dockerfile Copy the following text and paste it in to the editor.</description>
    </item>
    
    <item>
      <title>Lab - Run the App</title>
      <link>/workshops/source_to_image/run_the_app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/run_the_app/</guid>
      <description>Step 1 - Create a Route to the Gochat Service First we expose the pod with a service.
oc expose dc wetty --port=8080 --target-port=8080 --name=gochat Then we expose the service with a route.
oc expose svc gochat Step 2 - Download the App Source go get -d github.com/kevensen/openshift-gochat-client Step 3 - Build the App cd go/src/github.com/kevensen/openshift-gochat-clientglide install -v &amp;amp;&amp;amp; go install Step 4 - Annotate the Service Account to Use OpenShift Authorization These annotations allow for the Gochat Client to communicate to the OpenShift API for user credential verification.</description>
    </item>
    
    <item>
      <title>Lab - Source-to-Image</title>
      <link>/workshops/source_to_image/source_to_image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/source_to_image/</guid>
      <description>Step 1 - Create a new Project Space In the Wetty terminal, create a new project.
oc new-project s2i-userYOUR#  Step 2 - Go Get S2I Library cd ~go get github.com/openshift/source-to-image Step 3 - Build the S2I Tooling cd $GOPATH/src/github.com/openshift/source-to-imagehack/build-go.shexport PATH=$PATH:${GOPATH}/src/github.com/openshift/source-to-image/_output/local/bin/linux/amd64/ Step 4 - Create the S2I Project cd ~s2i create golang-s2i golang-s2i Now let&#39;s inspect the project directory
tree -a golang-s2i Step 5 - Edit the Dockerfile cd ~cat /dev/null &amp;gt; ~/golang-s2i/Dockerfilevi ~/golang-s2i/Dockerfile Copy the following text and paste it in to the editor.</description>
    </item>
    
    <item>
      <title>Lab - Strangle Your Monolith</title>
      <link>/workshops/strangling_the_monolith/lab2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/strangling_the_monolith/lab2/</guid>
      <description>Strangling the Monolith Design Pattern  Strangling - incrementally replacing functionality in app with something better (cheaper, faster, easier to maintain). As functionality is replaced, “dead” parts of monolith can be removed/retired. You can also wait for all functionality to be replaced before retiring anything! You can optionally include new functionality during strangulation to make it more attractive to business stakeholders.   [https://paulhammant.com/2013/07/14/legacy-application-strangulation-case-studies/](https://paulhammant.com/2013/07/14/legacy-application-strangulation-case-studies/)  Steps for Strangling the Monolith  1) Strangle Monolith</description>
    </item>
    
    <item>
      <title>Lab 01 - API Design</title>
      <link>/workshops/agile_integrations_ci/lab01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab01/</guid>
      <description>Lab 1 API Design Create an OpenAPI Specification using Apicurio Studio  Duration: 10 mins Audience: API Owner, Product Manager, Developers, Architects  
Overview As APIs become more widespread in the enterprise, consistent design and usage is critically important to improve reusability. The more reusable APIs are, the less friction there is for other internal or external teams to make progress. Having design standards and tools baked into the API development and maintenance process is a very powerful way to enable this consistency.</description>
    </item>
    
    <item>
      <title>Lab 01 - API Design</title>
      <link>/workshops/agile_integrations_dev/lab01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab01/</guid>
      <description>Lab 1 API Design Import an OpenAPI Specification using Apicurio Studio  Duration: 10 mins Audience: Developers, Architects  
Overview As APIs become more widespread in the enterprise, consistent design and usage is critically important to improve reusability. The more reusable APIs are, the less friction there is for other internal or external teams to make progress. Having design standards and tools baked into the API development and maintenance process is a very powerful way to enable this consistency.</description>
    </item>
    
    <item>
      <title>Lab 01 - Welcome to OpenShift</title>
      <link>/workshops/secure_software_factory/lab01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab01/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the console to help you get familiar with the user interface along with some key terminology we will use in subsequent lab content.
Key Terms We will be using the following terms throughout the workshop labs so here are some basic definitions you should be familiar with. You&#39;ll learn more terms along the way, but these are the basics to get you started.</description>
    </item>
    
    <item>
      <title>Lab 02 - API Mocking</title>
      <link>/workshops/agile_integrations_ci/lab02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab02/</guid>
      <description>Lab 2 API Mocking Bring your APIs to life  Duration: 20 mins Audience: Developers, Architects, Testers, Quality Engineers  
Overview When building and working with APIs, you often need to simulate the responses of the system before it has been fully completed. This is what we explore in this workshop - mocking up API structures quickly so they can be subjected to testing without having to create all the final service code.</description>
    </item>
    
    <item>
      <title>Lab 02 - API Mocking</title>
      <link>/workshops/agile_integrations_dev/lab02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab02/</guid>
      <description>Lab 2 API Mocking Bring your APIs to life  Duration: 20 mins Audience: Developers, Architects, Testers, Quality Engineers  
Overview When building and working with APIs, you often need to simulate the responses of the system before it has been fully completed. This is what we explore in this workshop - mocking up API structures quickly so they can be subjected to testing without having to create all the final service code.</description>
    </item>
    
    <item>
      <title>Lab 02 - Trusted Software Supply Chain</title>
      <link>/workshops/secure_software_factory/lab02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab02/</guid>
      <description>Trusted Software Supply Chain The notion of Trusted Software Supply Chain is that of applying an assembly line approach to software that includes building increasing levels of assurance before marking an artifact as ready to deploy to production.
In this workshop, you will use build orchestration tools such as Jenkins, Kubernetes, and OpenShift to support implementing and integrating continuous delivery (CD) as the foundation of automating the various stages of the deployment lifecycle are represented – from development, multiple types of testing, user acceptance, staging, to production release.</description>
    </item>
    
    <item>
      <title>Lab 03 - CI/CD Project and Pods</title>
      <link>/workshops/secure_software_factory/lab03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab03/</guid>
      <description>Before moving forward, it is important to understand the difference between Continuous Integration (CI), Continuous Delivery (CD), and Continuous Deployment.
Also, a part of this lab we’ll be using using Pipelines in OpenShift for CI/CD, which gives you control over building, deploying, and promoting your applications on OpenShift. Using a combination of the Jenkins Pipeline Build Strategy, Jenkinsfiles, and the OpenShift Domain Specific Language (DSL) (provided by the OpenShift Jenkins Client Plug-in), you can create advanced build, test, deploy, and promote pipelines for any scenario.</description>
    </item>
    
    <item>
      <title>Lab 03 - Fuse Online</title>
      <link>/workshops/agile_integrations_ci/lab03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab03/</guid>
      <description>Lab 3 Fuse Online  Duration: 20 mins Audience: Developers and Architects  
Overview When it comes to quick API development, you need both the integration experts as well as application developers to easily develop and deploy the APIs. Here is how to create a simple API with Fuse Online.
Why Red Hat? Red Hat Fuse integration solution empowers integration experts, application developers, and business users to engage in enterprise-wide collaboration and high-productivity self-service.</description>
    </item>
    
    <item>
      <title>Lab 03 - Swagger to REST</title>
      <link>/workshops/agile_integrations_dev/lab03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab03/</guid>
      <description>Lab 3 Swagger to REST Contract-first API development with a database interface, implemented using Eclipse Che  Duration: 20 mins Audience: Developers and Architects  
Overview In the context of defining API&#39;s, it&#39;s common for a Business Analyst (or Citizen Integrator) to first create an API specification or contract. By beginning the process with a clearly defined contract, a Developer can easily take the contract and auto-generate the underlying service to implement that API.</description>
    </item>
    
    <item>
      <title>Lab 04 - Jenkins and OpenShift</title>
      <link>/workshops/secure_software_factory/lab04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab04/</guid>
      <description>Jenkins can be integrated with OpenShift in 3 ways. Today, you&#39;ll be working with a containerized Jenkins Server that&#39;s fully integrated with OpenShift.
Login through SSO with Jenkins Go into your CI/CD project and find the running Jenkins Pod.
Click the external route (https://jenkins…) to go into your Jenkins Server
Click Login with OpenShift.
Login with your OpenShift Credentials. You maybe asked to accept authorizations. Go ahead and do so and re-login.</description>
    </item>
    
    <item>
      <title>Lab 04 - Managing API Endpoints</title>
      <link>/workshops/agile_integrations_ci/lab04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab04/</guid>
      <description>Lab 4 Managing API Endpoints Take control of your APIs  Duration: 15 mins Audience: API Owners, Product Managers, Developers, Architects  
Overview Once you have APIs deployed in your environment, it becomes critically important to manage who may use them, and for what purpose. You also need to begin to track usage of these different users to know who is or is not succeeding in their usage. For this reason, in this lab, you will be adding management capabilities to the API to give you control and visibility of it&#39;s usage.</description>
    </item>
    
    <item>
      <title>Lab 04 - SOAP to REST (Optional)</title>
      <link>/workshops/agile_integrations_dev/lab04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab04/</guid>
      <description>Lab 4 SOAP to REST (Optional) Contract-first API development wrapping an existing SOAP service, implemented using Eclipse Che  Duration: 20 mins Audience: Developers and Architects  
Overview Another important use case in developing API&#39;s is to take an existing legacy SOAP service and wrap it with a new RESTful endpoint. This SOAP to REST transformation is implemented in the API service layer (Fuse). This lab will walk you through taking an existing SOAP contract (WSDL), converting it to Java POJO&#39;s and exposing it using Camel REST DSL.</description>
    </item>
    
    <item>
      <title>Lab 05 - API Security</title>
      <link>/workshops/agile_integrations_ci/lab05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab05/</guid>
      <description>Lab 5 API Security Securing APIs with OpenID Connect and Red Hat Single Sign On  Duration: 20 mins Audience: API Owners, Product Managers, Developers, Architects  
Overview Once you have APIs in your organization and have applications being written, you also want to be sure in many cases that the various types of users of the APIs are correctly authenticated. In this lab you will discover how to set up the widely used OpenID connect pattern for Authentication.</description>
    </item>
    
    <item>
      <title>Lab 05 - Creating Your Pipeline</title>
      <link>/workshops/secure_software_factory/lab05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab05/</guid>
      <description>Today, you will be building your Trusted Software Supply Chain using a Jenkins Pipeline that is integrated with OpenShift.
In addition to standard Jenkins Pipeline Syntax, the OpenShift Jenkins image provides the OpenShift Domain Specific Language (DSL) (through the OpenShift Jenkins Client Plug-in), which aims to provide a readable, concise, comprehensive, and fluent syntax for rich interactions with an OpenShift API server, allowing for even more control over the build, deployment, and promotion of applications on your OpenShift cluster.</description>
    </item>
    
    <item>
      <title>Lab 05 - Fuse Online</title>
      <link>/workshops/agile_integrations_dev/lab05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab05/</guid>
      <description>Lab 5 Fuse Online  Duration: 20 mins Audience: Developers and Architects  
Overview When it comes to quick API development, you need both the integration experts as well as application developers to easily develop, deploy the APIs. Here is how to create a simple API with Fuse online.
Why Red Hat? Red Hat Fuse integration solution empowers integration experts, application developers, and business users to engage in enterprise-wide collaboration and high-productivity self-service.</description>
    </item>
    
    <item>
      <title>Lab 06 - API Developer Portal</title>
      <link>/workshops/agile_integrations_ci/lab06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab06/</guid>
      <description>Lab 6 API Developer Portal Publishing APIs to Developer Portal  Duration: 20 mins Audience: API Owners, Product Managers, Developers, Architects  
Overview The focal point of your developers experience is the API developer portal, and the level of effort you put into it will directly correlate to the level of success of your API program, and API developer engagement.
Why Red Hat? Red Hat 3scale API Management Solution provides a built-in, state-of-the-art CMS portal, making it very easy to create your own branded hub with a custom domain to manage developer interactions, and help increase API adoption.</description>
    </item>
    
    <item>
      <title>Lab 06 - Build App Stage</title>
      <link>/workshops/secure_software_factory/lab06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab06/</guid>
      <description>Add Build App Stage to Pipeline Text File Next, you will add the Build App Stage to your pipeline.
The git branch step will clone the openshift-tasks project with the git branch locally from your gogs server to your jenkins node. The jenkins node is leveraging the git jenkins plugin to communicate to gogs.
Please note that we are leveraging the eap-7 branch in our git project and not the master branch.</description>
    </item>
    
    <item>
      <title>Lab 06 - Managed API Endpoints</title>
      <link>/workshops/agile_integrations_dev/lab06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab06/</guid>
      <description>Lab 6 Managed API Endpoints Take control of your APIs  Duration: 5 mins Audience: API Owners, Product Managers, Developers, Architects  
Overview Once you have APIs deployed in your environment, it becomes critically important to manage who may use them and for what purpose. You also need to begin to track usage of these different users to know who is/is not succeeding in their usage. For this reason in this lab you will be adding management capabilities to the API to give you control and visibility of it&#39;s usage.</description>
    </item>
    
    <item>
      <title>Lab 07 - API Developer Portal</title>
      <link>/workshops/agile_integrations_dev/lab07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab07/</guid>
      <description>Lab 7 API Developer Portal Publishing APIs to Developer Portal  Duration: 20 mins Audience: API Owners, Product Managers, Developers, Architects  
Overview The focal point of your developers’ experience is the API developer portal, and the level of effort you put into it will determine the level of decreased support costs and increased developer engagement.
Why Red Hat? 3scale provides a built-in, state-of-the-art CMS portal, making it very easy to create your own branded hub with a custom domain to manage developer interactions and increase API adoption.</description>
    </item>
    
    <item>
      <title>Lab 07 - Test Stage</title>
      <link>/workshops/secure_software_factory/lab07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab07/</guid>
      <description>Add Test Stage Add the configuration for the Test Stage below to your pipeline text file.
Maven will run the test stage in the life cycle that we skipped at the previous stages.
Maven will place the test results in the surefire-reports folder. The maven surefire-reports plugin allows for the generation of reports for your unit tests.

Append to Jenkins Pipeline Configuration In Builds &amp;gt; Pipelines &amp;gt; tasks-pipeline &amp;gt; Actions &amp;gt; Edit</description>
    </item>
    
    <item>
      <title>Lab 07 - Testing Your Application</title>
      <link>/workshops/agile_integrations_ci/lab07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/lab07/</guid>
      <description>Lab 7 Testing the International Inc. Locations webpage Testing your web application  Duration: 10 mins Audience: API Consumers, Developers, Architects  
Overview This lab demonstrates the International Inc. Locations page using our newly authenticated locations API.
Why Red Hat? Applications can be built from many technologies. In this case we use a simple web application, but a wide range of Red Hat and non-Red Hat technologies could be used.</description>
    </item>
    
    <item>
      <title>Lab 08 - API Consumption</title>
      <link>/workshops/agile_integrations_dev/lab08/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/lab08/</guid>
      <description>Lab 8 API Consumption Connect Applications and APIs  Duration: 15 mins Audience: API Consumers, Developers, Architects  
Overview APIs provide the building blocks for applications, but it is applications which deliver functionality to the end users. hence to see APIs in action it helps to see how applications can call APIs to provide new functionality. In this lab you&#39;ll be able to create a simple web application which consumes the API you built earlier in the exercises.</description>
    </item>
    
    <item>
      <title>Lab 08 - Static Application Security Testing</title>
      <link>/workshops/secure_software_factory/lab08/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab08/</guid>
      <description>Static Application Security Testing Next we will add a Code Analysis Stage into the pipeline.
We will leverage the Maven Sonar plugin to run SonarQube scanning against our source code.
SonarQube is an open source static code analysis tool that we can use to automate running security scans against your source code to further improve the security of your application. Every time you check-in code, SonarQube will scan the quality and perform a threat analysis of that code.</description>
    </item>
    
    <item>
      <title>Lab 09 - Archive App</title>
      <link>/workshops/secure_software_factory/lab09/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab09/</guid>
      <description>Add Archive Stage Archiving the built and tested application into a trusted repository is important to making sure you are building with trusted parts. You assume this application is built properly and all the previous stages have passed. With that confidence, your built and tested application should be immutable in a trusted repository. The repository will version or audit any changes to the application, configuration, and dependencies.
Add Archive Stage Steps into your pipeline.</description>
    </item>
    
    <item>
      <title>Lab 1 - Login to Horizon</title>
      <link>/workshops/rhosp_101/2_login_to_horizon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/2_login_to_horizon/</guid>
      <description>Welcome to Red Hat OpenStack Platform (RHOSP)! This lab provides a quick tour of the Horizon dashboard to help you get familiar with the user interface. If you are already familiar with the basics of Horizon simply ensure you can login and have access to your student project.
Accessing Horizon RHOSP provides a web dashboard that allows you to perform various tasks via a web browser. Let&#39;s get started by logging into Horizon and checking the status of the platform.</description>
    </item>
    
    <item>
      <title>Lab 1 - Welcome</title>
      <link>/workshops/openshift_101_dcmetromap/lab1-welcome/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab1-welcome/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the console to help you get familiar with the user interface along with some key terminology we will use in subsequent lab content. If you are already familiar with the basics of OpenShift simply ensure you can login and create the project.
Key Terms We will be using the following terms throughout the workshop labs so here are some basic definitions you should be familiar with.</description>
    </item>
    
    <item>
      <title>Lab 1 - Welcome</title>
      <link>/workshops/openshift_4_101/lab1-welcome/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab1-welcome/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the OpenShift console to help you get familiar with the user interface. If you are already familiar with the basics of OpenShift, this will be easy in that we are simply ensuring you can login and create a project.
Accessing OpenShift OpenShift provides a web console that allows you to perform various tasks via a web browser.
Let&#39;s Login to the Web Console  Use your browser to navigate to the URI provided by your instructor and login with the user/password provided.</description>
    </item>
    
    <item>
      <title>Lab 1 - Welcome</title>
      <link>/workshops/openshift_4_101_dynatrace/lab1-welcome/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab1-welcome/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the console to help you get familiar with the user interface along with some key terminology we will use in subsequent lab content. If you are already familiar with the basics of OpenShift simply ensure you can login and create the project.
Key Terms We will be using the following terms throughout the workshop labs so here are some basic definitions you should be familiar with.</description>
    </item>
    
    <item>
      <title>Lab 1.0 - Workshop introduction</title>
      <link>/workshops/security_container_intro/lab01-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab01-intro/</guid>
      <description>Table of Contents Important Conventions used in this lab Getting access to the servers     This lab session is a low-level, hands-on introduction to container security using the container tools included with Red Hat Enterprise Linux 8. It is intended to be consumed as a series of self paced exercises.
 Prerequisites   An introductory knowledge of Linux and containers is helpful.
  Basic text editing skills using vim or nano.</description>
    </item>
    
    <item>
      <title>Lab 10 - Create Image Builder</title>
      <link>/workshops/secure_software_factory/lab10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab10/</guid>
      <description>Containers Containers are an important foundation for your application in building a Trusted Software Supply Chain. You want a secure and blessed golden container image that your application will inherit security controls from.
Containers are built using a layered approach. For example, to create a container of a Java web application, you could do so in multiple layers: the OS, the JVM, the web server, and the code itself.</description>
    </item>
    
    <item>
      <title>Lab 10 - Labels (Optional)</title>
      <link>/workshops/openshift_4_101_dynatrace/lab10-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab10-labels/</guid>
      <description>Labels This is a pretty simple lab, we are going to explore labels. You can use labels to organize, group, or select API objects.
For example, pods are &amp;quot;tagged&amp;quot; with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different docker containers as related entities.
Labels on a pod In a previous lab we added our web app using a S2I template.</description>
    </item>
    
    <item>
      <title>Lab 11 - Build Image</title>
      <link>/workshops/secure_software_factory/lab11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab11/</guid>
      <description>Add Build Image Stage Add the Build Image Stage into your pipeline.
The &amp;quot;sh&amp;quot; are shell commands in your pipeline that are executed on the Jenkins Slave. Maven built the openshift-tasks.war in it the target directory and it will be copied into the created oc-builds directory.
The startBuild stage is kicked off from Openshift and pointing to the local directory &amp;quot;oc-build&amp;quot; on the Jenkins slave.

Append to Jenkins Pipeline Configuration In Builds &amp;gt; Pipelines &amp;gt; tasks-pipeline &amp;gt; Actions &amp;gt; Edit</description>
    </item>
    
    <item>
      <title>Lab 11 - Webhooks and Rollbacks (Optional)</title>
      <link>/workshops/openshift_4_101_dynatrace/lab11-cicd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab11-cicd/</guid>
      <description>Build Triggers, Webhooks and Rollbacks - Oh My! Once you have an app deployed in OpenShift you can take advantage of some continuous capabilities that help to enable DevOps and automate your management process. We will cover some of those in this lab: Build triggers, webhooks, and rollbacks.
A bit of configuration We are going to do some integration and coding with an external git repository. For this lab we are going to use github, if you don&#39;t already have an account, you can create one here.</description>
    </item>
    
    <item>
      <title>Lab 12 - Blue | Green Deployment (Optional)</title>
      <link>/workshops/openshift_4_101_dynatrace/lab12-bluegreen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab12-bluegreen/</guid>
      <description>Blue/Green deployments When implementing continuous delivery for your software one very useful technique is called Blue/Green deployments. It addresses the desire to minimize downtime during the release of a new version of an application to production. Essentially, it involves running two production versions of your app side-by-side and then switching the routing from the last stable version to the new version once it is verified. Using OpenShift, this can be very seamless because using containers we can easily and rapidly deploy a duplicate infrastructure to support alternate versions and modify routes as a service.</description>
    </item>
    
    <item>
      <title>Lab 12 - Create and Deploy to Dev</title>
      <link>/workshops/secure_software_factory/lab12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab12/</guid>
      <description>Add Create Dev Stage Add Create Dev Stage into the pipeline.
We first check if an deployment config for the Dev Project already exists. If it does not exists, a new application is created and deployment config is defined for the Dev Project.
Before a trigger is created, the pipeline sleeps for 10 seconds. A deployment configuration can contain triggers, which drive the creation of new deployment processes in response to events inside the cluster.</description>
    </item>
    
    <item>
      <title>Lab 13 - Promote and Deploy to Stage</title>
      <link>/workshops/secure_software_factory/lab13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab13/</guid>
      <description>Add Promote to Stage Enter the Promote to STAGE below into your pipeline.
We set an approval to promote to the application to the Stage Project. The approval process is a good feature for various gates of your deployments. We also set a 15 minute timeout on the approval. You also tag the tasks image with latest and the version from the pom file.

Append to Jenkins Pipeline Configuration In Builds &amp;gt; Pipelines &amp;gt; tasks-pipeline &amp;gt; Actions &amp;gt; Edit</description>
    </item>
    
    <item>
      <title>Lab 14 - Run Pipeline</title>
      <link>/workshops/secure_software_factory/lab14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab14/</guid>
      <description>Verify Completed Pipeline Before we kick off your pipeline, let&#39;s verify it.
In Builds &amp;gt; Pipelines &amp;gt; tasks-pipeline &amp;gt; Actions &amp;gt; Edit YAML
Take a look and see if it matches the below text. If not, please correct it.
apiVersion: v1 kind: BuildConfig metadata: annotations: pipeline.alpha.openshift.io/uses: &amp;#39;[{&amp;#34;name&amp;#34;: &amp;#34;jenkins&amp;#34;, &amp;#34;namespace&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;kind&amp;#34;: &amp;#34;DeploymentConfig&amp;#34;}]&amp;#39; labels: app: cicd-pipeline name: cicd-pipeline name: tasks-pipeline spec: triggers: - type: GitHub github: secret: &amp;#34;secret101&amp;#34; - type: Generic generic: secret: &amp;#34;secret101&amp;#34; runPolicy: Serial source: type: None strategy: jenkinsPipelineStrategy: env: - name: DEV_PROJECT value: dev-userYOUR# - name: STAGE_PROJECT value: stage-userYOUR# jenkinsfile: |- def version, mvnCmd = &amp;#34;mvn -s configuration/cicd-settings-nexus3.</description>
    </item>
    
    <item>
      <title>Lab 15 - Trigger the Software Supply Chain</title>
      <link>/workshops/secure_software_factory/lab15/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab15/</guid>
      <description>Trigger the Trusted Software Supply Chain with Code Check-ins Through automation, you will trigger the Trusted Software Supply Chain with code check-ins.

Verify Webhook in Gogs The Webhook is what triggers your pipeline upon code check-ins. You want your SCM to trigger the pipeline as opposed to Jenkins constantly polling the source code for changes.
Click on the Gogs route url in the CI/CD project which takes you to the home page.</description>
    </item>
    
    <item>
      <title>Lab 16 - Create Quay Account</title>
      <link>/workshops/secure_software_factory/lab16/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab16/</guid>
      <description>Create Quay Account Please go to the Quay Login with the link given to you by your instructor
Click Create Account at the bottom of the page.


Fill in the account information for Quay.
 Username: your assigned username i.e userYOUR# E-mail address: your email address Password: openshift Confirm Password: openshift  Click Create Account



 .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.</description>
    </item>
    
    <item>
      <title>Lab 17 - Clair Vulnerability Scan</title>
      <link>/workshops/secure_software_factory/lab17/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab17/</guid>
      <description>Add Clair Container Scan Add the configuration for the Container Vulnerability Scan below to your pipeline text file.


Append to Jenkins Pipeline Configuration In Builds &amp;gt; Pipelines &amp;gt; tasks-pipeline &amp;gt; Actions &amp;gt; Edit
In your pipeline, add and update the following variables after the version and mvnCmd definitions. Please fill in the values between the quotes.
 ocuser : the openshift user given to you by your insturctor ocpass : the openshift password given to you by your insturctor ocp : the openshift host given to you by your instuctor quayuser : the quay user you created previously quaypass : the quay password you created previously quayrepo : the quay repo you will push your app image to i.</description>
    </item>
    
    <item>
      <title>Lab 18 - OpenSCAP DISA STIG Scan</title>
      <link>/workshops/secure_software_factory/lab18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/lab18/</guid>
      <description>Add OpenSCAP DISA STIG Scan Add the configuration for the OpenSCAP Scan below to your pipeline text file.


Append to Jenkins Pipeline Configuration In Builds &amp;gt; Pipelines &amp;gt; tasks-pipeline &amp;gt; Actions &amp;gt; Edit
In your pipeline, verify the following variables are there after the version and mvnCmd definitions. Please fill in the values between the quotes if not done previously.
 ocuser : the openshift user given to you by your insturctor ocpass : the openshift password given to you by your insturctor ocp : the openshift host given to you by your instuctor</description>
    </item>
    
    <item>
      <title>Lab 2 - BYO Container</title>
      <link>/workshops/openshift_4_101/lab2-byocontainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab2-byocontainer/</guid>
      <description>Bring your own OCI image It&#39;s easy to get started with OpenShift whether you&#39;re using our app templates or bringing your existing assets. In this quick lab we will deploy an application using an exisiting container image. OpenShift will create an image stream for the image as well as deploy and manage containers based on that image.
Let&#39;s point OpenShift to an existing built container image Choose either to complete the steps via CLI or Web Console.</description>
    </item>
    
    <item>
      <title>Lab 2 - BYO Container</title>
      <link>/workshops/openshift_4_101_dynatrace/lab2-byocontainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab2-byocontainer/</guid>
      <description>Bring your own docker It&#39;s easy to get started with OpenShift whether you&#39;re using our app templates or bringing your existing assets. In this quick lab we will deploy an application using an exisiting container image. OpenShift will create an image stream for the image as well as deploy and manage containers based on that image. And we will dig into the details to show how all that works.</description>
    </item>
    
    <item>
      <title>Lab 2 - BYO Docker</title>
      <link>/workshops/openshift_101_dcmetromap/lab2-byodocker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab2-byodocker/</guid>
      <description>Bring your own docker It&#39;s easy to get started with OpenShift whether you&#39;re using our app templates or bringing your existing docker assets. In this quick lab we will deploy an application using an exisiting docker image. OpenShift will create an image stream for the image as well as deploy and manage containers based on that image. And we will dig into the details to show how all that works.</description>
    </item>
    
    <item>
      <title>Lab 2 - Managing Neutron Networks</title>
      <link>/workshops/rhosp_101/3_managing_networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/3_managing_networks/</guid>
      <description>Neutron - Overlay Networking for OpenStack Neutron is an OpenStack project to provide &amp;quot;networking as a service&amp;quot; between interface devices (e.g., vNICs) managed by other Openstack services (e.g., nova).
Starting in the Folsom release, Neutron is a core and supported part of the OpenStack platform.
Examining Project Network Topology  Navigate to Network -&amp;gt; Network Topology using the second level navigation tabs.
   Lab 2 Figure 1: Neutron Network Topology   There are two ways to view the project network layout.</description>
    </item>
    
    <item>
      <title>Lab 2.0 - Container registries</title>
      <link>/workshops/security_container_intro/lab02-registry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab02-registry/</guid>
      <description>Table of Contents Configuring the registries Node 1 Testing the registries Node 2 Testing the registries Configuring and testing the bastion     During this module you will configure both nodes to host secure container registries. The registry software runs in a container and listens on port 5000. Configuring the registry consists of generating a self-signed SSL certificate and running the container. Two simple bash scripts are provided to do this.</description>
    </item>
    
    <item>
      <title>Lab 2.1 - Reviewing container details</title>
      <link>/workshops/openshift_4_101/lab2.1-byocontainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab2.1-byocontainer/</guid>
      <description>Now that we have a running container, we can browse our project details with the command line  https://console-openshift-console.apps.example.redhatgov.io/terminal 
  Try typing the following to see what is available to &#39;get&#39;:
 $ oc project demo-YOUR# $ oc get all   Now let&#39;s look at what our image stream has in it:
 $ oc get is$ oc describe is/nexus  An image stream can be used to automatically perform an action, such as updating a deployment, when a new image, in our case a new version of the nexus image, is created.</description>
    </item>
    
    <item>
      <title>Lab 2.2 - Clean-up &amp; Summary</title>
      <link>/workshops/openshift_4_101/lab2.2-byocontainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab2.2-byocontainer/</guid>
      <description>Terminal access  https://console-openshift-console.apps.example.redhatgov.io/terminal 
Let&#39;s clean this up   Let&#39;s clean up all this to get ready for the next lab:
 $ oc project demo-YOUR# $ oc delete all --selector app=nexus Summary In this lab, you&#39;ve deployed an example container image into a pod running in OpenShift. You exposed a route for clients to access that service via their web browsers. And you learned how to get and describe resources using the command line and the web console.</description>
    </item>
    
    <item>
      <title>Lab 3 - Deploying an App with S2I</title>
      <link>/workshops/openshift_101_dcmetromap/lab3-s2i/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab3-s2i/</guid>
      <description>Source to Image (S2I) One of the useful components of OpenShift is its source-to-image capability. S2I is a framework that makes it easy to turn your source code into runnable images. The main advantage of using S2I for building reproducible docker images is the ease of use for developers. You&#39;ll see just how simple it can be in this lab.
Let&#39;s build a node.js web server using S2I We can do this either via the command line or the web console.</description>
    </item>
    
    <item>
      <title>Lab 3 - Deploying an App with S2I</title>
      <link>/workshops/openshift_4_101/lab3-s2i/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab3-s2i/</guid>
      <description>Source to Image (S2I) One of the useful components of OpenShift is its source-to-image capability. S2I is a framework that makes it easy to turn your source code into runnable images. The main advantage of using S2I for building reproducible docker images is the ease of use for developers. You&#39;ll see just how simple it can be in this lab.
Let&#39;s build a node.js web app, using S2I We can do this either via the command line or the web console.</description>
    </item>
    
    <item>
      <title>Lab 3 - Install Dynatrace Agent</title>
      <link>/workshops/openshift_4_101_dynatrace/lab3-installagent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab3-installagent/</guid>
      <description>Deploy the Dynatrace OneAgent Operator on OpenShift 4.4+  Prerequisites: 1) If no existing account, sign-up for a trial with Dynatrace at https://www.dynatrace.com/trial/ 2) Dynatrace API Token from Settings  Integration  Dynatrace API  Generate Token Activate the following settings: * Access problem and event feed, metrics, and topology * Read log content * Write configuration 3) Dynatrace PaaS token PaaS (used to download OneAgent and ActiveGate installers) from Settings  Integration  Platform as a Service  Generate Token 4) apiUrl - URL to the API of your Dynatrace environment.</description>
    </item>
    
    <item>
      <title>Lab 3 - Managing Floating IP Addresses</title>
      <link>/workshops/rhosp_101/4_managing_floating_ip_addresses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/4_managing_floating_ip_addresses/</guid>
      <description>Floating IP vs. Private IP Addresses Private IP Address A private IP address is assigned to an instance&#39;s network-interface by the DHCP server. The address is visible from within the instance by using a command like “ip a”. The address is typically part of a private network and is used for communication between instances in the same broadcast domain via virtual switch (L2 agent on each compute node). It can also be accessible from instances in other private networks via virtual router (L3 agent).</description>
    </item>
    
    <item>
      <title>Lab 3.0 - Intro to Podman and base images</title>
      <link>/workshops/security_container_intro/lab03-podman/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab03-podman/</guid>
      <description>Table of Contents What is a pod? The Universal Base Image Tagging and pushing images to a remote registry Working with root and rootless containers. Stopping and removing containers Container process information User Namespace Support Auditing containers UID Mapping Challenge References     What is a pod? A pod is a group of one or more containers with shared storage, network and a specification for how to run the containers.</description>
    </item>
    
    <item>
      <title>Lab 3.1 - Check out the build details</title>
      <link>/workshops/openshift_4_101/lab3.1-s2i/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab3.1-s2i/</guid>
      <description>Check out the build details We can see the details of what the S2I builder did. This can be helpful to diagnose issues if builds are failing.
 TIP: For a node.js app, running &amp;quot;npm shrinkwrap&amp;quot; is a good practice to perform on your branch before releasing changes that you plan to build/deploy as an image with S2I
CLI Steps    Terminal access  https://console-openshift-console.apps.example.redhatgov.io/terminal    Goto the terminal and type the following:  $ oc get builds  Note the name of your build from the above command output and use it to see the logs with:  $ oc logs builds/[BUILD_NAME] The console will print out the full log for your build.</description>
    </item>
    
    <item>
      <title>Lab 4 - Configuring Dynatrace ActiveGate (Optional)</title>
      <link>/workshops/openshift_4_101_dynatrace/lab4-configdynatraceactivegate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab4-configdynatraceactivegate/</guid>
      <description>Connect your OpenShift clusters to Dynatrace with an ActiveGate to take advantage of the dedicated OpenShift overview page Connecting your OpenShift clusters to Dynatrace to take advantage of the dedicated OpenShift overview page requires that you run an ActiveGate in your environment (version 1.163+).
See https://www.dynatrace.com/support/help/technology-support/cloud-platforms/openshift/monitoring/monitor-openshift-clusters-with-dynatrace/
oc project dynatrace oc apply -f https://www.dynatrace.com/support/help/codefiles/kubernetes/kubernetes-monitoring-service-account.yaml serviceaccount/dynatrace-monitoring created clusterrole.rbac.authorization.k8s.io/dynatrace-monitoring-cluster created clusterrolebinding.rbac.authorization.k8s.io/dynatrace-monitoring-cluster createdoc config view --minify -o jsonpath=&amp;#39;{.clusters[0].cluster.server}&amp;#39; https://api.ocp4.local:6443  Get the Bearer token for the OpenShift cluster using the following command: Copy the secret returned by the following command:  oc get secret $(oc get sa dynatrace-monitoring -o jsonpath=&amp;#39;{.</description>
    </item>
    
    <item>
      <title>Lab 4 - Developing and Managing Your Application</title>
      <link>/workshops/openshift_101_dcmetromap/lab4-devmanage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab4-devmanage/</guid>
      <description>Developing and managing an application in OpenShift In this lab we will explore some of the common activities undertaken by developers working in OpenShift. You will become familiar with how to use environment variables, secrets, build configurations, and more. Let&#39;s look at some of the basic things a developer might care about for a deployed app.
Setup From the previous lab you should have the DC Metro Maps web app running in OpenShift.</description>
    </item>
    
    <item>
      <title>Lab 4 - Developing and Managing Your Application</title>
      <link>/workshops/openshift_4_101/lab4-devmanage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab4-devmanage/</guid>
      <description>Developing and managing an application in OpenShift In this lab we will explore some of the common activities undertaken by developers working in OpenShift. You will become familiar with how to use environment variables, secrets, build configurations, and more. Let&#39;s look at some of the basic things a developer might care about for a deployed app.
Setup From the previous lab you should have the DC Metro Maps web app running in OpenShift.</description>
    </item>
    
    <item>
      <title>Lab 4 - Managing Security Groups</title>
      <link>/workshops/rhosp_101/5_managing_security_groups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/5_managing_security_groups/</guid>
      <description>Security Groups in OpenStack A security group is a named collection of network access rules that are used to limit the types of traffic that have access to instances. When you launch an instance, you can assign one or more security groups to it. If you do not create security groups, new instances are automatically assigned to the default security group, unless you explicitly specify a different security group.</description>
    </item>
    
    <item>
      <title>Lab 4.0 - Linux kernel capabilities</title>
      <link>/workshops/security_container_intro/lab04-isolation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab04-isolation/</guid>
      <description>Table of Contents Capabilities Exploring the capabilities of containers. Examining container processes Capabilities Challenge #1 Capabilities Challenge #2 Capabilities Challenge #3 Example Solutions to Challenges     Containers provide a certain degree of process isolation via kernel namespaces. In this module, we’ll examine the capabilities of a process running in a containerized namespace. Specifically we&amp;#8217;ll look at how Linux capabilities can be used to grant a particular or subset of root privileges to a process running in a container.</description>
    </item>
    
    <item>
      <title>Lab 5 - Deploying an App with S2I</title>
      <link>/workshops/openshift_4_101_dynatrace/lab5-s2i/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab5-s2i/</guid>
      <description>Source to Image (S2I) One of the useful components of OpenShift is its source-to-image capability. S2I is a framework that makes it easy to turn your source code into runnable images. The main advantage of using S2I for building reproducible docker images is the ease of use for developers. You&#39;ll see just how simple it can be in this lab.
Let&#39;s build a node.js web app, using S2I We can do this either via the command line or the web console.</description>
    </item>
    
    <item>
      <title>Lab 5 - Managing Images</title>
      <link>/workshops/rhosp_101/6_managing_images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/6_managing_images/</guid>
      <description>Where Shall We Store Our Golden Images? Glance The Image service (glance) project provides a service where users can upload and discover data assets that are meant to be used with other services. This currently includes images and metadata definitions.
Glance image services include discovering, registering, and retrieving virtual machine (VM) images. Glance has a RESTful API that allows querying of VM image metadata as well as retrieval of the actual image.</description>
    </item>
    
    <item>
      <title>Lab 5 - Webhooks and Rollbacks</title>
      <link>/workshops/openshift_101_dcmetromap/lab5-rollbacks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab5-rollbacks/</guid>
      <description>Build Triggers, Webhooks and Rollbacks - Oh My! Once you have an app deployed in OpenShift you can take advantage of some continuous capabilities that help to enable DevOps and automate your management process. We will cover some of those in this lab: Build triggers, webhooks, and rollbacks.
A bit of configuration We are going to do some integration and coding with an external git repository. For this lab we are going to use github, if you don&#39;t already have an account, you can create one here.</description>
    </item>
    
    <item>
      <title>Lab 5 - Webhooks and Rollbacks</title>
      <link>/workshops/openshift_4_101/lab5-rollbacks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab5-rollbacks/</guid>
      <description>Build Triggers, Webhooks and Rollbacks - Oh My! Once you have an app deployed in OpenShift you can take advantage of some continuous capabilities that help to enable DevOps and automate your management process. We will cover some of those in this lab: Build triggers, webhooks, and rollbacks.
A bit of configuration We are going to do some integration and coding with an external git repository. For this lab we are going to use github, if you don&#39;t already have an account, you can create one here.</description>
    </item>
    
    <item>
      <title>Lab 5.0 - SELinux container integration</title>
      <link>/workshops/security_container_intro/lab05-selinux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab05-selinux/</guid>
      <description>Table of Contents !Namespaced Warmup Exercise Bind Mounts and Labeling Private Mounts Shared Mounts Read-Only Containers     In this section, we’ll cover the basics of SELinux and containers. SELinux policy prevents a lot of break out situations where the other security mechanisms fail. By default, podman processes are labeled with container_runtime_t and they are prevented from doing (almost) all SELinux operations. But processes within containers do not know that they are running within a container.</description>
    </item>
    
    <item>
      <title>Lab 6 - Application Replication</title>
      <link>/workshops/openshift_4_101/lab6-replicationrecovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab6-replicationrecovery/</guid>
      <description>Things will go wrong, and that&#39;s why we have replication and recovery Things will go wrong with your software, or your hardware, or from something completely out of your control. But, we can plan for such failures, thus minimizing their impact. OpenShift supports this via the replication and recovery functionality.
Replication Let&#39;s walk through a simple example of how the replication controller can keep your deployment at a desired state.</description>
    </item>
    
    <item>
      <title>Lab 6 - Deploying a 3 Tier App</title>
      <link>/workshops/openshift_4_101_dynatrace/lab6-deploy3tierapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab6-deploy3tierapp/</guid>
      <description>Deploying a 3 Tier App This builds on the work done in the labs so far to deploy a 3 Tier App. The reason for this lab are to highlight the capabilities of OpenShift to handle more complex application stacks and to show how Dynatrace automatically captures new applications and interactions between them.
 Instructions are located in a Red Hat blog post at:  https://developers.redhat.com/articles/deploy-a-NodeJS-app-OpenShift   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.</description>
    </item>
    
    <item>
      <title>Lab 6 - Managing Instances</title>
      <link>/workshops/rhosp_101/7_managing_instances/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/7_managing_instances/</guid>
      <description>Now We are Ready to Launch an OpenStack Instance First, What is an Instance? Instances are virtual machines that run inside the OpenStack cloud. You can launch an instance from the following sources:
 Images uploaded to the Image service
 Image that you have copied to a persistent volume. The instance launches from the volume, which is provided by the cinder-volume API Instance snapshot that you took  Second.</description>
    </item>
    
    <item>
      <title>Lab 6 - Replication and Recovery</title>
      <link>/workshops/openshift_101_dcmetromap/lab6-replicationrecovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab6-replicationrecovery/</guid>
      <description>Things will go wrong, and that&#39;s why we have replication and recovery Things will go wrong with your software, or your hardware, or from something out of your control. But we can plan for that failure, and planning for it let&#39;s us minimize the impact. OpenShift supports this via what we call replication and recovery.
Replication Let&#39;s walk through a simple example of how the replication controller can keep your deployment at a desired state.</description>
    </item>
    
    <item>
      <title>Lab 6.0 - Container inspection with Podman &amp; Skopeo</title>
      <link>/workshops/security_container_intro/lab06-inspecting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab06-inspecting/</guid>
      <description>Table of Contents Podman inspect Podman diff Podman live mounts Working with Skopeo Inspecting images     Container images can easily be pulled from any public registry and run on a container host but is this good practice? Can you trust this image and what are its contents? A better practice would be to inspect the image before running it.
 Podman inspect Use podman to inspect the registry.</description>
    </item>
    
    <item>
      <title>Lab 6.1 - Application Recovery</title>
      <link>/workshops/openshift_4_101/lab6.1-replicationrecovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab6.1-replicationrecovery/</guid>
      <description>Recovery Okay, now that we have a slightly more interesting replication state, we can test a service outages scenario. In this scenario, the dc-metro-map replication controller will ensure that other pods are created to replace those that become unhealthy. Let&#39;s forcibly inflict an issue and see how OpenShift responds.
CLI Steps    Terminal access  https://console-openshift-console.apps.example.redhatgov.io/terminal    Choose a random pod and delete it:  $ oc get pods $ oc delete pod/PODNAME $ oc get pods -w If you&#39;re fast enough you&#39;ll see the pod you deleted go &amp;quot;Terminating&amp;quot; and you&#39;ll also see a new pod immediately get created and transition from &amp;quot;Pending&amp;quot; to &amp;quot;Running&amp;quot;.</description>
    </item>
    
    <item>
      <title>Lab 6.2 - Application Health</title>
      <link>/workshops/openshift_4_101/lab6.2-replicationrecovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab6.2-replicationrecovery/</guid>
      <description>Application Health In addition to the health of your application&#39;s pods, OpenShift will watch the containers inside those pods. Let&#39;s forcibly inflict some issues and see how OpenShift responds.
CLI Steps    Terminal access  https://console-openshift-console.apps.example.redhatgov.io/terminal    Choose a running pod and shell into it:  $ oc get pods $ oc exec PODNAME -it /bin/bash You are now executing a bash shell running in the container of the pod.</description>
    </item>
    
    <item>
      <title>Lab 7 - Developing and Managing Your Application</title>
      <link>/workshops/openshift_4_101_dynatrace/lab7-devmanage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab7-devmanage/</guid>
      <description>Developing and managing an application in OpenShift In this lab we will explore some of the common activities undertaken by developers working in OpenShift. You will become familiar with how to use environment variables, secrets, build configurations, and more. Let&#39;s look at some of the basic things a developer might care about for a deployed app.
Setup From the previous lab you should have the DC Metro Maps web app running in OpenShift.</description>
    </item>
    
    <item>
      <title>Lab 7 - Labels</title>
      <link>/workshops/openshift_101_dcmetromap/lab7-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab7-labels/</guid>
      <description>Labels This is a pretty simple lab, we are going to explore labels. You can use labels to organize, group, or select API objects.
For example, pods are &amp;quot;tagged&amp;quot; with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different docker containers as related entities.
Labels on a pod In a previous lab we added our web app using a S2I template.</description>
    </item>
    
    <item>
      <title>Lab 7 - Labels</title>
      <link>/workshops/openshift_4_101/lab7-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab7-labels/</guid>
      <description>Labels This is a pretty simple lab, we are going to explore labels. You can use labels to organize, group, or select API objects.
For example, pods are &amp;quot;tagged&amp;quot; with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different docker containers as related entities.
Labels on a pod In a previous lab we added our web app using a S2I template.</description>
    </item>
    
    <item>
      <title>Lab 7 - Using the OpenStack CLI(s)</title>
      <link>/workshops/rhosp_101/8_using_the_cli/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/8_using_the_cli/</guid>
      <description>Okay. UIs Are Cool and All, But Can&#39;t I Just Use the CLI? Absolutely! There are 3 ways to interact with OpenStack.
 API
 CLI
 GUI  In this lab, we are going to walk through some of the activities that we did in the previous labs via the CLI.
 When you see a CLI command starting with **openstack**, the command is part of the Unified CLI.</description>
    </item>
    
    <item>
      <title>Lab 7.0 - Container image signing</title>
      <link>/workshops/security_container_intro/lab07-signing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab07-signing/</guid>
      <description>Table of Contents Configuring the signature claim store Generating key pairs Working with trust policies Pulling signed images Create a trust policy for Red Hat images. Blocking a registry     In this lab, you will use podman to sign container images and create trust policies to control access to registries.
 To begin the image signing procedure, a signature claim is generated by encrypting a container image manifest using a private gpg key.</description>
    </item>
    
    <item>
      <title>Lab 8 - CI / CD Pipeline</title>
      <link>/workshops/openshift_101_dcmetromap/lab8-cicd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab8-cicd/</guid>
      <description>CI/CD Defined In modern software projects many teams utilize the concept of Continuous Integration (CI) and Continuous Delivery (CD). By setting up a tool chain that continuously builds, tests, and stages software releases, a team can ensure that their product can be reliably released at any time. OpenShift can be an enabler in the creation and management of this tool chain.
In this lab we walk through creating a simple example of a CI/CD pipeline utlizing Jenkins, all running on top of OpenShift!</description>
    </item>
    
    <item>
      <title>Lab 8 - CI / CD Pipeline</title>
      <link>/workshops/openshift_4_101/lab8-cicd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab8-cicd/</guid>
      <description>CI/CD Defined In modern software projects many teams utilize the concept of Continuous Integration (CI) and Continuous Delivery (CD). By setting up a tool chain that continuously builds, tests, and stages software releases, a team can ensure that their product can be reliably released at any time. OpenShift can be an enabler in the creation and management of this tool chain.
In this lab we walk through creating a simple example of a CI/CD [pipeline] utilizing Jenkins, all running on top of OpenShift!</description>
    </item>
    
    <item>
      <title>Lab 8 - Using Heat to Describe Stacks</title>
      <link>/workshops/rhosp_101/9_heat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/9_heat/</guid>
      <description>We Have Done a Ton of Stuff, But All Manually. How Do We Orchestrate? In OpenStack, the answer to that is Heat.
Heat is the main project for orchestration in OpenStack. It implements an orchestration engine to launch multiple composite cloud applications based on templates in the form of text files that can be treated like code. A native Heat template format is evolving, but Heat also endeavours to provide compatibility with the AWS CloudFormation template format, so that many existing CloudFormation templates can be launched on OpenStack.</description>
    </item>
    
    <item>
      <title>Lab 8 - Webhooks and Rollbacks (Optional)</title>
      <link>/workshops/openshift_4_101_dynatrace/lab8-rollbacks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab8-rollbacks/</guid>
      <description>Build Triggers, Webhooks and Rollbacks - Oh My! Once you have an app deployed in OpenShift you can take advantage of some continuous capabilities that help to enable DevOps and automate your management process. We will cover some of those in this lab: Build triggers, webhooks, and rollbacks.
A bit of configuration We are going to do some integration and coding with an external git repository. For this lab we are going to use github, if you don&#39;t already have an account, you can create one here.</description>
    </item>
    
    <item>
      <title>Lab 8.0 - Composing containers with Buildah</title>
      <link>/workshops/security_container_intro/lab08-builds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_container_intro/lab08-builds/</guid>
      <description>Table of Contents Building from Universal Base Images Build using Docker (BuD)   This module will introduce Buildah, a command line tool for building Open Container Initiative compatible container images. Buildah is easy to incorporate into scripts and build pipelines, and best of all, it doesn&amp;#8217;t require a running container daemon to build its image. You can read more about Buildah at the buildah.io web site. Also, have a look at the buildah chapter in the RHEL 8 documentation.</description>
    </item>
    
    <item>
      <title>Lab 9 - Blue | Green Deployment</title>
      <link>/workshops/openshift_101_dcmetromap/lab9-bluegreen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/lab9-bluegreen/</guid>
      <description>Blue/Green deployments When implementing continuous delivery for your software one very useful technique is called Blue/Green deployments. It addresses the desire to minimize downtime during the release of a new version of an application to production. Essentially, it involves running two production versions of your app side-by-side and then switching the routing from the last stable version to the new version once it is verified. Using OpenShift, this can be very seamless because using containers we can easily and rapidly deploy a duplicate infrastructure to support alternate versions and modify routes as a service.</description>
    </item>
    
    <item>
      <title>Lab 9 - Blue | Green Deployment</title>
      <link>/workshops/openshift_4_101/lab9-bluegreen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/lab9-bluegreen/</guid>
      <description>Blue/Green deployments When implementing continuous delivery for your software one very useful technique is called Blue/Green deployments. It addresses the desire to minimize downtime during the release of a new version of an application to production. Essentially, it involves running two production versions of your app side-by-side and then switching the routing from the last stable version to the new version once it is verified. Using OpenShift, this can be very seamless because using containers we can easily and rapidly deploy a duplicate infrastructure to support alternate versions and modify routes as a service.</description>
    </item>
    
    <item>
      <title>Lab 9 - Managing Block and Object Storage</title>
      <link>/workshops/rhosp_101/10_storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhosp_101/10_storage/</guid>
      <description>Working With Block and Object Storage in OpenStack Block Storage is Handled by Cinder The OpenStack Block Storage service works through the interaction of a series of daemon processes named cinder-* that reside persistently on the host machine or machines. You can run all the binaries from a single node, or spread across multiple nodes. You can also run them on the same node as other OpenStack services.
To administer the OpenStack Block Storage service, it is helpful to understand a number of concepts.</description>
    </item>
    
    <item>
      <title>Lab 9 - Replication and Recovery (Optional)</title>
      <link>/workshops/openshift_4_101_dynatrace/lab9-replicationrecovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/lab9-replicationrecovery/</guid>
      <description>Things will go wrong, and that&#39;s why we have replication and recovery Things will go wrong with your software, or your hardware, or from something out of your control. But we can plan for that failure, and planning for it let&#39;s us minimize the impact. OpenShift supports this via what we call replication and recovery.
Replication Let&#39;s walk through a simple example of how the replication controller can keep your deployment at a desired state.</description>
    </item>
    
    <item>
      <title>Login Tour - Butterfly</title>
      <link>/workshops/openshift_101_dcmetromap/login_tour_wetty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/login_tour_wetty/</guid>
      <description>Accessing OpenShift OpenShift provides a web console that allows you to perform various tasks via a web browser.
Let&#39;s Login to the Web Console  Use your browser to navigate to the URI provided by your instructor and login with the user/password provided.
 https://master.example.redhatgov.io:8443  Login Webpage
Once logged in you should see your available projects - or a button to create a project if none exist already.</description>
    </item>
    
    <item>
      <title>Login Tour - Wetty</title>
      <link>/workshops/secure_software_factory/login_tour_wetty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/login_tour_wetty/</guid>
      <description>Introduction to Wetty (Browser-based SSH) This lab provides a quick tour of the browser based SSH client Wetty. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
Accessing Wetty Ask your instructor for the wetty URL.
Login Info login: userYOUR# Password: &amp;lt;Instructor Provided&amp;gt; After logging in, you should see a shell.
The wetty instance will already have the &#39;oc&#39; command installed on them.</description>
    </item>
    
    <item>
      <title>Microservices</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab1.1_welcome/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab1.1_welcome/</guid>
      <description>A Brief Introduction to Microservices Microservices, also known as the microservice architecture, is a software development technique that structures an application as a collection of loosely coupled services. Microservice architectures enable the continuous delivery/deployment/scaling of complex applications.
Why microservices? Agility. Deliver application updates faster. Isolate and fix bugs easier. Done right, a microservices architecture will you help to meet several important non-functional requirements for your software:
 scalability performance reliability resiliency extensibility availability  What is a Service Mesh?</description>
    </item>
    
    <item>
      <title>Observability - Feature Update</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab3.1_featureupdate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab3.1_featureupdate/</guid>
      <description>Digging into Observability Istio provides additional capabilities to analyze the service mesh and its performance. Let&#39;s deploy a new version of the user profile service and analyze its effect on the service mesh.
Feature Update The code has already been written for you on the &#39;workshop-feature-update&#39; branch of the repo.
  Create a new build on this feature branch:  oc new-app -f ./openshift-configuration/userprofile-build.yaml \ -p APPLICATION_NAME=userprofile \ -p APPLICATION_CODE_URI=https://github.</description>
    </item>
    
    <item>
      <title>Observability - Grafana</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab3.2_grafana/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab3.2_grafana/</guid>
      <description>Service Mesh Metrics with Grafana Grafana is a monitoring tool that can be integrated with Istio for metric observation. Using Grafana, you can look at metrics associated with services in your mesh. Let&#39;s use Grafana to get more information about the user profile service.
Explore Grafana First, let&#39;s explore the Grafana user interface.
  Open the Grafana console. Retrieve the endpoint for Grafana:  GRAFANA_CONSOLE=$(oc get route grafana -n istio-system --template=&amp;#39;https://{{.</description>
    </item>
    
    <item>
      <title>Observability - Jaeger</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab3.3_jaeger/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab3.3_jaeger/</guid>
      <description>Distributed Tracing with Jaeger Jaeger is a distributed tracing tool that lets you trace requests as they flow through your service mesh. This is incredibly useful for debugging performance issues in your microservices architecture.
Explore Jaeger First, let&#39;s explore the Jaeger user interface.
  Open the Jaeger console. Retrieve the endpoint for Jaeger:  JAEGER_CONSOLE=$(oc get route jaeger -n istio-system --template=&amp;#39;https://{{.spec.host}}&amp;#39;) echo $JAEGER_CONSOLE  Click &#39;Allow selected permissions&#39; if prompted to authorized access.</description>
    </item>
    
    <item>
      <title>Observability - Kiali</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab2.3_kiali/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab2.3_kiali/</guid>
      <description>Introducing Kiali for Observability All of your microservices are running in the service mesh. Now, you need a way to visualize the service mesh topology. That is, what&#39;s running in your service mesh and how are they connected?
Istio provides Kiali, an open source project that gives you a console view of your service mesh. You can inspect the health of your service mesh, and it has further integrations for metric querying and tracing that we will cover in later labs.</description>
    </item>
    
    <item>
      <title>Prerequisites</title>
      <link>/workshops/source_to_image/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/prerequisites/</guid>
      <description>Getting Started If you choose to work from your own command line, instead of the web terminal (Wetty) environment, that&#39;s just fine! You&#39;ll have to install a few of utilities first.
Install oc Click here for instructions on how to install the OpenShift Container Platform CLI.
Install git Click here for instructions on how to install Git on your workstation.
Install golang Click here for instructions on how to install Golang.</description>
    </item>
    
    <item>
      <title>Prerequisites</title>
      <link>/workshops/strangling_the_monolith/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/strangling_the_monolith/prerequisites/</guid>
      <description>If we provided you a laptop or cloud environment. We already set all this up for you — feel free to skip everything below!  Install oc Click here for instructions on how to install the OpenShift Container Platform CLI.
Install git Click here for instructions on how to install Git on your workstation.
Create GitHub account Click here for instructions on how to setup an account.</description>
    </item>
    
    <item>
      <title>Prerequisites for JDV Development</title>
      <link>/workshops/jdv_dev/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/prerequisites/</guid>
      <description> What you need  A free account on Red Hat Developers to download JBoss Data Virtualization and JBoss Developer Studio A Java Development Kit (JDK) 8 git available as a local command Node.js runtime, 6.x or higher Postman for testing An accessible PostgreSQL instance  Recommended: Download a pre-built VM, ISO or docker image from TurnKey Linux. The provided VM/ISO also provides an web interface for loading data. Step by step instructions are provided for a Turnkey linux instance   </description>
    </item>
    
    <item>
      <title>Security - Auth Policy</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab5.4_authpolicy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab5.4_authpolicy/</guid>
      <description>Authorizing and Authenticating Access via Policy In the previous labs we secured and verified the service-to-service communication. But what about user-to-service communication (aka origin authentication)? The service mesh can help with that too. To do this, we need to bring in an identity provider that will issue JSON Web Tokens (JWTs) to signed in users. JWTs are an open, industry standard (RFC 7519) way of sharing identity. The app-ui service will pass those JWTs along with it&#39;s API requests.</description>
    </item>
    
    <item>
      <title>Security - Auth Policy Prep</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab5.3_prep/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab5.3_prep/</guid>
      <description>Single Sign-On Login, registration, and role-based authorization is handled via Red Hat SSO (aka Keycloak). SSO is included as part of OpenShift or any middleware product subscription you have from Red Hat - woot!
Installation Because the service mesh will leverage SSO to authenticate users and generate JWT we need to install it. You will install it via Operator into your namespace.
Customize the Resources Check out the .yaml files here and customize for this workshop cluster&#39;s domain, and your user project.</description>
    </item>
    
    <item>
      <title>Security - mTLS</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab5.1_intromtls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab5.1_intromtls/</guid>
      <description>Mutual TLS One of the key features of the Service Mesh is its ability to bring additional security to your applications. It does this in a several different ways that will be explored in the next few labs. The first of which is a concept known as &amp;quot;Mutual TLS&amp;quot; or mTLS for short.
Imagine a scenario where you are deploying a microservices application and are expecting a lot of PII to flow between the services.</description>
    </item>
    
    <item>
      <title>Security - Securing Ingress and Egress</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab5.5_secureingressegress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab5.5_secureingressegress/</guid>
      <description>Securing Ingress and Egress In cases where strict security is required we need to configure specifics around securing ingress and egress traffic. Security around egress is often used to lock down and deny access to potentially harmful resources outside the network. Additionally, it is a good practice to prevent malicious activities from originating from the cluster.
You are probably already familiar with basic ingress security concepts. Essentially, only exposing particular services to be accessible from outside the cluster and using basic TLS/SSL.</description>
    </item>
    
    <item>
      <title>Security - Verifying mTLS</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab5.2_verifymtls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab5.2_verifymtls/</guid>
      <description>Checking and Verification of mTLS OK, now that all our services are encrypting traffic Let&#39;s take a look in Kiali to see our encryption setup.
  Open up the dashboard to Kiali (if you don&#39;t already have it open) and navigate to the Graph view. In the first drop down select the &#34;Service graph&#34; and in the &#34;Display&#34; drop down make sure the &#34;Security&#34; check box is checked.  You should see something like this screenshot with little locks indicating that mTLS is working for service-to-service communication.</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/setup/</guid>
      <description>In order for you to &#39;write&#39; your first bit of Ansible, you will need an editor &amp;#8230;&amp;#8203; Ansible playbooks are written in YAML and you can use an text editor &#34;like vim&#34; or an editor like Visual Studio Code to get started. For this workshop, we will be utilizing a web-based IDE (integrated development environment) call Red Hat CodeReady Workspaces. Red Hat CodeReady Workspaces is a developer tool that makes cloud-native development practical for teams, using Kubernetes and containers to provide any member of the development or IT team with a consistent, preconfigured development environment.</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/ansible_automation/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/setup/</guid>
      <description>What you will learn: Red Hat Web Console - Terminal This lab provides a quick tour of the browser based Terminal via the Red Hat Web Console. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Red Hat Enterprise Linux Web Console (cockpit) Use this URL to access the Web Console node, just change the workshop name (if applicable).</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/ansible_tower_azure/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/setup/</guid>
      <description>What you will learn: Wetty (Browser-based SSH) This lab provides a quick tour of the browser based SSH client Wetty. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Wetty Use this URL to access the Wetty node, just change the workshopname and region (if applicable). Ask your instructor for the workshopname and region.</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/ansible_tower_intro/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_intro/setup/</guid>
      <description>What you will learn: Red Hat Web Console - Terminal This lab provides a quick tour of the browser based Terminal via the Red Hat Web Console. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Ansible Tower Use this URL to access the Ansible Tower interface. Your instructor will provide the password, and the user ID is admin:</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/containers_101/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/setup/</guid>
      <description>What you will learn: Red Hat Web Console - Terminal This lab provides a quick tour of the browser based Terminal via the Red Hat Web Console. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Red Hat Enterprise Linux Web Console (cockpit) Use this URL to access the Web Console node, just change the workshop name (if applicable).</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab1.3_installing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab1.3_installing/</guid>
      <description>Setup You will conduct these labs in an OpenShift cluster. As in the previous terminal deployment exercise, you can access your cluster via console and CLI.
OpenShift You will use the OpenShift &#39;oc&#39; CLI to execute commands for the majority of this lab.
  Login using API endpoint and remember to add the &#39;--insecure-skip-tls-verify=true&#39; flag  $ oc login https://api.example.redhatgov.io:6443 --insecure-skip-tls-verify=true Authentication required for https://api.example.redhatgov.io:6443 (openshift) Username: userYOUR# Password: Login successful.</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/rhel_8/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/setup/</guid>
      <description>What you will learn: Red Hat Web Console - Terminal This lab provides a quick tour of the browser based Terminal via the Red Hat Web Console. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Red Hat Enterprise Linux Web Console (cockpit) Use this URL to access the Web Console node, just change the workshop name (if applicable).</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/security_containers/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/setup/</guid>
      <description>What you will learn: Red Hat Web Console - Terminal This lab provides a quick tour of the browser based Terminal via the Red Hat Web Console. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Red Hat Enterprise Linux Web Console (cockpit) Use this URL to access the Web Console node, just change the workshop name (if applicable).</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/security_openshift/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/setup/</guid>
      <description>Introduction to Wetty (Browser-based SSH) This lab provides a quick tour of the browser based SSH client Wetty. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Wetty Use this URL to access the Wetty node, just change the workshopname. Ask your instructor for the workshopname.
 https://&amp;lt;workshopname&amp;gt;.master.&amp;lt;student number&amp;gt;.redhatgov.io:8888   After logging in, you should see a shell.</description>
    </item>
    
    <item>
      <title>Setup</title>
      <link>/workshops/selinux_policy/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/setup/</guid>
      <description>What you will learn: Cockpit (Browser-based Admin Interface) This lab provides a quick tour of the browser based admin interface client &#39;Cockpit&#39;. To help you get familiar with lab environment along with some key terminology we will use in subsequent lab content.
   Accessing Cockpit Use this URL to access the Cockpit node, just change the workshop name (if applicable). Ask your instructor for the workshop name.</description>
    </item>
    
    <item>
      <title>The Setup</title>
      <link>/workshops/source_to_image/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/source_to_image/setup/</guid>
      <description>Step 1 - Log in to the Web User Interface The workshop moderator will provide you with the URL, your username, and password.
OpenShift WebUI Login        Step 2 - Create a Project for Your Wetty Terminal Create a new project by clicking the &amp;quot;New Project&amp;quot; button in the upper right of the WebUI.
Create a Project        Step 3 - Name Your Project Name your project as follows.</description>
    </item>
    
    <item>
      <title>Traffic Control - Circuit Breaking</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab4.4_circuitbreaking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab4.4_circuitbreaking/</guid>
      <description>Circuit Breaking Your Services Fault Injection lets you see how the service mesh behaves when there are failures in network calls to a specific service. But how do you protect a service if it has overloaded or failing instances serving traffic? Ideally, you would like to identify an instance that is failing and prevent clients from connecting to it once it meets a certain threshold.
In OpenShift, an instance is equivalent to a Kubernetes pod running the microservice.</description>
    </item>
    
    <item>
      <title>Traffic Control - Fault Injection</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab4.3_faultinjection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab4.3_faultinjection/</guid>
      <description>Testing Resiliency with Fault Injection Your application is working great now with the new version of the user profile service. But the previous version caused performance issues, and future updates may cause issues in other areas of the application. How can you test how your application behaves when a failure occurs?
You need a way to simulate failure in the service mesh. By doing so, you can test if your application functions correctly in a degraded state.</description>
    </item>
    
    <item>
      <title>Traffic Control - Routing Traffic</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab4.1_routingtraffic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab4.1_routingtraffic/</guid>
      <description>Basics on Routing Traffic Through the Mesh One of Istio&#39;s core capabilities is the ability to dynamically control how services communicate without modifying the application code itself. The general concept is called Traffic Management. It allows you to do things such as A/B test, canary rollouts, rollbacks, and more.
The two core API objects for traffic management are the Virtual Service and the Destination Rule. The destination rule is for the owner of a microservice - what versions do I expose and what happens to traffic before it reaches my service?</description>
    </item>
    
    <item>
      <title>Traffic Control - Traffic Splitting</title>
      <link>/workshops/openshift_service_mesh_v1.0/lab4.2_trafficsplitting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_service_mesh_v1.0/lab4.2_trafficsplitting/</guid>
      <description>Splitting Traffic Amongst Service Versions It&#39;s time to fix the performance issue of the application. Previously, you deployed a new version of the application and routed 100% of traffic to the new version. This time, you&#39;ll use Istio traffic routing to do canary rollouts and split traffic.
Feature Fix The code to fix the performance issue of the user profile service has already been written for you on the &#39;workshop-feature-fix&#39; branch.</description>
    </item>
    
    <item>
      <title>Verify Prerequisites</title>
      <link>/workshops/agile_integrations_ci/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/prerequisites/</guid>
      <description> Prerequisites  This workshop is done entirely through the browser.   Chrome is the preferred browser for this workshop  </description>
    </item>
    
    <item>
      <title>Verify Prerequisites</title>
      <link>/workshops/agile_integrations_dev/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/prerequisites/</guid>
      <description> Prerequisites  This workshop is done entirely through the browser.   Chrome is the preferred browser for this workshop  </description>
    </item>
    
    <item>
      <title>Verify Prerequisites</title>
      <link>/workshops/openshift_101_dcmetromap/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_101_dcmetromap/prerequisites/</guid>
      <description>Prerequisites  This workshop is done entirely through the browser, but you do need a free personal GitHub account.  Create a Github account If you don&#39;t have a person GitHub account please sign up here to create a free account.
 Create GitHub Account Download Git Client (optional)   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.</description>
    </item>
    
    <item>
      <title>Verify Prerequisites</title>
      <link>/workshops/openshift_4_101/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101/prerequisites/</guid>
      <description>Prerequisites  This workshop is done entirely through the browser, but you do need a free personal GitHub account.  Create a Github account If you don&#39;t have a person GitHub account please sign up here to create a free account.
 Create GitHub Account   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.4); -webkit-animation-name: fadeIn; -webkit-animation-duration: 0.</description>
    </item>
    
    <item>
      <title>Verify Prerequisites</title>
      <link>/workshops/openshift_4_101_dynatrace/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/openshift_4_101_dynatrace/prerequisites/</guid>
      <description>Prerequisites  This workshop is done entirely through the browser, but you do need a free personal GitHub account.  Create a Github account If you don&#39;t have a person GitHub account please sign up here to create a free account.
 Create GitHub Account Download Git Client (optional)   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.</description>
    </item>
    
    <item>
      <title>Verify Prerequisites</title>
      <link>/workshops/secure_software_factory/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/secure_software_factory/prerequisites/</guid>
      <description>Prerequisites  This workshop is done entirely through the browser.   Chrome is the preferred browser for this workshop   .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgb(0,0,0); background-color: rgba(0,0,0,0.4); -webkit-animation-name: fadeIn; -webkit-animation-duration: 0.4s; animation-name: fadeIn; animation-duration: 0.4s } .modal-content { position: fixed; bottom: 0; background-color: #fefefe; width: 100%; -webkit-animation-name: slideIn; -webkit-animation-duration: 0.4s; animation-name: slideIn; animation-duration: 0.</description>
    </item>
    
    <item>
      <title>Welcome to OpenShift</title>
      <link>/workshops/agile_integrations_ci/openshift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_ci/openshift/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the console to help you get familiar with the user interface along with some key terminology we will use in subsequent lab content.
Key Terms We will be using the following terms throughout the workshop labs so here are some basic definitions you should be familiar with. You&#39;ll learn more terms along the way, but these are the basics to get you started.</description>
    </item>
    
    <item>
      <title>Welcome to OpenShift</title>
      <link>/workshops/agile_integrations_dev/openshift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/agile_integrations_dev/openshift/</guid>
      <description>Welcome to OpenShift! This lab provides a quick tour of the console to help you get familiar with the user interface along with some key terminology we will use in subsequent lab content.
Key Terms We will be using the following terms throughout the workshop labs so here are some basic definitions you should be familiar with. You&#39;ll learn more terms along the way, but these are the basics to get you started.</description>
    </item>
    
    <item>
      <title>Workshop is Done</title>
      <link>/workshops/cloudforms41/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/cloudforms41/wrapup/</guid>
      <description>That&#39;s it! Hopefully, these labs provided you some idea of how to perform common tasks within the CloudForms environment. And hopefully, you have a deeper understanding of how CloudForms simplifies IT, providing unified management and operations in a hybrid environment.
As your IT infrastructure progresses from traditional virtualization toward an Infrastructure-as-a-Service (IaaS) model, CloudForms evolves, protecting your investments and providing consistent user experience and functionality.
Please feel free to continue to &amp;quot;kick the tires&amp;quot; in the demo environment we&#39;ve setup and explore the web console.</description>
    </item>
    
    <item>
      <title>Workshop is done!</title>
      <link>/workshops/jdv_dev/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/jdv_dev/wrapup/</guid>
      <description> Congratulations  You imported 3 different disparate datasources and exposed them as data services. You also combined them into a federated view and applied role based security. Congratulations!  Additional resources  JDV Documentation Teiid, upstream project for JDV odata, documentation for REST data services. JDV uses odata(v2) and odata(v4). V4 must be called with the context of /odata4 Teiid Examples. Github with various source examples  </description>
    </item>
    
    <item>
      <title>Wrap Up</title>
      <link>/workshops/security_containers/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_containers/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about Container Security that you can apply in your daily role.
 What do you think? How can we help you understand Container Security better?
 Before you leave, check out the Resources page that is part of this guide. There you can find a ton of links that will further your Container education.
 This Participant Guide will remain active for the next two weeks.</description>
    </item>
    
    <item>
      <title>Wrap Up</title>
      <link>/workshops/security_openshift/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/security_openshift/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about OpenShift Security that you can apply in your daily role.
 What do you think? How can we help you understand OpenShift Security better?
 Before you leave, check out the Resources page that is part of this guide. There you can find a ton of links that will further your OpenShift education.
 This Participant Guide will remain active for the next two weeks.</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/alpha/ansible_with_rhocp_edge/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/alpha/ansible_with_rhocp_edge/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about Ansible and Red Hat Ansible Tower that you can apply in your daily role.
 What do you think? How can we help you understand Ansible Tower better?
 Before you leave, check out the Resources page that is part of this guide. There, you can find a ton of links that will further your Ansible education.</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/ansible_automation/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_automation/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about Ansible and Red Hat Ansible Tower that you can apply in your daily role.
 What do you think? How can we help you understand Ansible Tower better?
 Before you leave, check out the Resources page that is part of this guide. There, you can find a ton of links that will further your Ansible education.</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/ansible_tower_azure/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_azure/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about Ansible and Red Hat Ansible Tower that you can apply in your daily role.
 What do you think? How can we help you understand Ansible Tower better?
 Before you leave, check out the Resources page that is part of this guide. There, you can find a ton of links that will further your Ansible education.</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/ansible_tower_intro/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/ansible_tower_intro/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about Ansible and Red Hat Ansible Tower that you can apply in your daily role.
 What do you think? How can we help you understand Ansible Tower better?
 Before you leave, check out the Resources page that is part of this guide. There, you can find a ton of links that will further your Ansible education.</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/containers_101/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_101/wrapup/</guid>
      <description>Wrap Up and Q&amp;amp;A That wraps up what we have planned for today.
 What do you think? How can we better help you understand Linux container concepts?
 For more in depth overview and examples for working with containers and RHEL check out this link.
 Thank you for your time and participation!
   </description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/containers_the_hard_way/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/containers_the_hard_way/wrapup/</guid>
      <description>Wrap Up and Q&amp;amp;A That wraps up what we have planned for today.
 We explored Linux Namespaces and the unshare command in great depth. Obviously container enginers make heavy use of these namespaces. Their job is to orchestrate the namespaces, OverlayFS, image files, and active containers so you don&amp;#8217;t have to. Obviously, you wouldn&amp;#8217;t run containers this way in production, but there may be times that you&amp;#8217;d like to make use of namespaces.</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/rhel_8/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/rhel_8/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve enjoyed learning about all the goodness that is RHEL 8!
 What do you think? How can we help you understand new RHEL features better?
 This Participant Guide will remain active for the next two weeks. Please take advantage of it by downloading RHEL8 and installing a test host or VM back at your organization.
 Thank you for your time and participation!</description>
    </item>
    
    <item>
      <title>Wrapup</title>
      <link>/workshops/selinux_policy/wrapup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workshops/selinux_policy/wrapup/</guid>
      <description>That wraps up what we have planned for today. We hope you&amp;#8217;ve learned something valuable about SELinux policies that you can apply in your daily role.
 What do you think? How can we help you understand SELinux better?
 Before you leave, check out the Resources page that is part of this guide. There, you can find a ton of links that will further your SELinux education.
 This Participant Guide will remain active for the next two weeks.</description>
    </item>
    
  </channel>
</rss>